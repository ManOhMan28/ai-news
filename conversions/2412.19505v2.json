{
  "schema_name": "DoclingDocument",
  "version": "1.0.0",
  "name": "2412.19505v2",
  "origin": {
    "mimetype": "application/pdf",
    "binary_hash": 10361672130197538581,
    "filename": "2412.19505v2.pdf"
  },
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/texts/0"
      },
      {
        "$ref": "#/texts/1"
      },
      {
        "$ref": "#/texts/2"
      },
      {
        "$ref": "#/texts/3"
      },
      {
        "$ref": "#/texts/4"
      },
      {
        "$ref": "#/texts/5"
      },
      {
        "$ref": "#/pictures/0"
      },
      {
        "$ref": "#/texts/16"
      },
      {
        "$ref": "#/texts/17"
      },
      {
        "$ref": "#/texts/18"
      },
      {
        "$ref": "#/texts/19"
      },
      {
        "$ref": "#/texts/20"
      },
      {
        "$ref": "#/texts/21"
      },
      {
        "$ref": "#/texts/22"
      },
      {
        "$ref": "#/texts/23"
      },
      {
        "$ref": "#/texts/24"
      },
      {
        "$ref": "#/texts/25"
      },
      {
        "$ref": "#/texts/26"
      },
      {
        "$ref": "#/texts/27"
      },
      {
        "$ref": "#/texts/28"
      },
      {
        "$ref": "#/texts/29"
      },
      {
        "$ref": "#/texts/30"
      },
      {
        "$ref": "#/texts/31"
      },
      {
        "$ref": "#/texts/32"
      },
      {
        "$ref": "#/texts/33"
      },
      {
        "$ref": "#/texts/34"
      },
      {
        "$ref": "#/texts/35"
      },
      {
        "$ref": "#/texts/36"
      },
      {
        "$ref": "#/texts/37"
      },
      {
        "$ref": "#/texts/38"
      },
      {
        "$ref": "#/texts/39"
      },
      {
        "$ref": "#/texts/40"
      },
      {
        "$ref": "#/texts/41"
      },
      {
        "$ref": "#/texts/42"
      },
      {
        "$ref": "#/texts/43"
      },
      {
        "$ref": "#/texts/44"
      },
      {
        "$ref": "#/texts/45"
      },
      {
        "$ref": "#/texts/46"
      },
      {
        "$ref": "#/texts/47"
      },
      {
        "$ref": "#/texts/48"
      },
      {
        "$ref": "#/texts/49"
      },
      {
        "$ref": "#/texts/50"
      },
      {
        "$ref": "#/texts/51"
      },
      {
        "$ref": "#/pictures/1"
      },
      {
        "$ref": "#/texts/89"
      },
      {
        "$ref": "#/texts/90"
      },
      {
        "$ref": "#/texts/91"
      },
      {
        "$ref": "#/texts/92"
      },
      {
        "$ref": "#/texts/93"
      },
      {
        "$ref": "#/texts/94"
      },
      {
        "$ref": "#/texts/95"
      },
      {
        "$ref": "#/texts/96"
      },
      {
        "$ref": "#/texts/97"
      },
      {
        "$ref": "#/texts/98"
      },
      {
        "$ref": "#/texts/99"
      },
      {
        "$ref": "#/texts/100"
      },
      {
        "$ref": "#/texts/101"
      },
      {
        "$ref": "#/texts/102"
      },
      {
        "$ref": "#/texts/103"
      },
      {
        "$ref": "#/texts/104"
      },
      {
        "$ref": "#/texts/105"
      },
      {
        "$ref": "#/texts/106"
      },
      {
        "$ref": "#/pictures/2"
      },
      {
        "$ref": "#/texts/127"
      },
      {
        "$ref": "#/texts/128"
      },
      {
        "$ref": "#/texts/129"
      },
      {
        "$ref": "#/texts/130"
      },
      {
        "$ref": "#/texts/131"
      },
      {
        "$ref": "#/texts/132"
      },
      {
        "$ref": "#/texts/133"
      },
      {
        "$ref": "#/texts/134"
      },
      {
        "$ref": "#/texts/135"
      },
      {
        "$ref": "#/texts/136"
      },
      {
        "$ref": "#/texts/137"
      },
      {
        "$ref": "#/texts/138"
      },
      {
        "$ref": "#/texts/139"
      },
      {
        "$ref": "#/texts/140"
      },
      {
        "$ref": "#/texts/141"
      },
      {
        "$ref": "#/texts/142"
      },
      {
        "$ref": "#/texts/143"
      },
      {
        "$ref": "#/texts/144"
      },
      {
        "$ref": "#/texts/145"
      },
      {
        "$ref": "#/texts/146"
      },
      {
        "$ref": "#/texts/147"
      },
      {
        "$ref": "#/texts/148"
      },
      {
        "$ref": "#/texts/149"
      },
      {
        "$ref": "#/tables/0"
      },
      {
        "$ref": "#/texts/150"
      },
      {
        "$ref": "#/texts/151"
      },
      {
        "$ref": "#/texts/152"
      },
      {
        "$ref": "#/texts/153"
      },
      {
        "$ref": "#/texts/154"
      },
      {
        "$ref": "#/texts/155"
      },
      {
        "$ref": "#/texts/156"
      },
      {
        "$ref": "#/pictures/3"
      },
      {
        "$ref": "#/texts/167"
      },
      {
        "$ref": "#/tables/1"
      },
      {
        "$ref": "#/tables/2"
      },
      {
        "$ref": "#/texts/168"
      },
      {
        "$ref": "#/texts/169"
      },
      {
        "$ref": "#/tables/3"
      },
      {
        "$ref": "#/texts/170"
      },
      {
        "$ref": "#/texts/171"
      },
      {
        "$ref": "#/texts/172"
      },
      {
        "$ref": "#/texts/173"
      },
      {
        "$ref": "#/texts/174"
      },
      {
        "$ref": "#/texts/175"
      },
      {
        "$ref": "#/texts/176"
      },
      {
        "$ref": "#/texts/177"
      },
      {
        "$ref": "#/texts/178"
      },
      {
        "$ref": "#/pictures/4"
      },
      {
        "$ref": "#/texts/196"
      },
      {
        "$ref": "#/pictures/5"
      },
      {
        "$ref": "#/texts/207"
      },
      {
        "$ref": "#/tables/4"
      },
      {
        "$ref": "#/texts/208"
      },
      {
        "$ref": "#/texts/209"
      },
      {
        "$ref": "#/tables/5"
      },
      {
        "$ref": "#/texts/210"
      },
      {
        "$ref": "#/texts/211"
      },
      {
        "$ref": "#/texts/212"
      },
      {
        "$ref": "#/texts/213"
      },
      {
        "$ref": "#/texts/214"
      },
      {
        "$ref": "#/texts/215"
      },
      {
        "$ref": "#/texts/216"
      },
      {
        "$ref": "#/texts/217"
      },
      {
        "$ref": "#/groups/0"
      },
      {
        "$ref": "#/texts/232"
      },
      {
        "$ref": "#/groups/1"
      },
      {
        "$ref": "#/texts/262"
      },
      {
        "$ref": "#/groups/2"
      },
      {
        "$ref": "#/texts/268"
      },
      {
        "$ref": "#/texts/269"
      },
      {
        "$ref": "#/texts/270"
      },
      {
        "$ref": "#/groups/3"
      },
      {
        "$ref": "#/texts/276"
      },
      {
        "$ref": "#/texts/277"
      },
      {
        "$ref": "#/texts/278"
      },
      {
        "$ref": "#/texts/279"
      },
      {
        "$ref": "#/texts/280"
      },
      {
        "$ref": "#/texts/281"
      },
      {
        "$ref": "#/texts/282"
      },
      {
        "$ref": "#/texts/283"
      },
      {
        "$ref": "#/texts/284"
      },
      {
        "$ref": "#/texts/285"
      },
      {
        "$ref": "#/texts/286"
      },
      {
        "$ref": "#/texts/287"
      },
      {
        "$ref": "#/texts/288"
      },
      {
        "$ref": "#/texts/289"
      },
      {
        "$ref": "#/texts/290"
      },
      {
        "$ref": "#/texts/291"
      },
      {
        "$ref": "#/texts/292"
      },
      {
        "$ref": "#/texts/293"
      },
      {
        "$ref": "#/tables/6"
      },
      {
        "$ref": "#/texts/294"
      },
      {
        "$ref": "#/texts/295"
      },
      {
        "$ref": "#/texts/296"
      },
      {
        "$ref": "#/texts/297"
      },
      {
        "$ref": "#/texts/298"
      },
      {
        "$ref": "#/texts/299"
      },
      {
        "$ref": "#/texts/300"
      },
      {
        "$ref": "#/texts/301"
      },
      {
        "$ref": "#/texts/302"
      },
      {
        "$ref": "#/texts/303"
      },
      {
        "$ref": "#/texts/304"
      },
      {
        "$ref": "#/tables/7"
      },
      {
        "$ref": "#/tables/8"
      },
      {
        "$ref": "#/texts/305"
      },
      {
        "$ref": "#/texts/306"
      },
      {
        "$ref": "#/texts/307"
      }
    ],
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/218"
        },
        {
          "$ref": "#/texts/219"
        },
        {
          "$ref": "#/texts/220"
        },
        {
          "$ref": "#/texts/221"
        },
        {
          "$ref": "#/texts/222"
        },
        {
          "$ref": "#/texts/223"
        },
        {
          "$ref": "#/texts/224"
        },
        {
          "$ref": "#/texts/225"
        },
        {
          "$ref": "#/texts/226"
        },
        {
          "$ref": "#/texts/227"
        },
        {
          "$ref": "#/texts/228"
        },
        {
          "$ref": "#/texts/229"
        },
        {
          "$ref": "#/texts/230"
        },
        {
          "$ref": "#/texts/231"
        }
      ],
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/233"
        },
        {
          "$ref": "#/texts/234"
        },
        {
          "$ref": "#/texts/235"
        },
        {
          "$ref": "#/texts/236"
        },
        {
          "$ref": "#/texts/237"
        },
        {
          "$ref": "#/texts/238"
        },
        {
          "$ref": "#/texts/239"
        },
        {
          "$ref": "#/texts/240"
        },
        {
          "$ref": "#/texts/241"
        },
        {
          "$ref": "#/texts/242"
        },
        {
          "$ref": "#/texts/243"
        },
        {
          "$ref": "#/texts/244"
        },
        {
          "$ref": "#/texts/245"
        },
        {
          "$ref": "#/texts/246"
        },
        {
          "$ref": "#/texts/247"
        },
        {
          "$ref": "#/texts/248"
        },
        {
          "$ref": "#/texts/249"
        },
        {
          "$ref": "#/texts/250"
        },
        {
          "$ref": "#/texts/251"
        },
        {
          "$ref": "#/texts/252"
        },
        {
          "$ref": "#/texts/253"
        },
        {
          "$ref": "#/texts/254"
        },
        {
          "$ref": "#/texts/255"
        },
        {
          "$ref": "#/texts/256"
        },
        {
          "$ref": "#/texts/257"
        },
        {
          "$ref": "#/texts/258"
        },
        {
          "$ref": "#/texts/259"
        },
        {
          "$ref": "#/texts/260"
        },
        {
          "$ref": "#/texts/261"
        }
      ],
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/263"
        },
        {
          "$ref": "#/texts/264"
        },
        {
          "$ref": "#/texts/265"
        },
        {
          "$ref": "#/texts/266"
        },
        {
          "$ref": "#/texts/267"
        }
      ],
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/271"
        },
        {
          "$ref": "#/texts/272"
        },
        {
          "$ref": "#/texts/273"
        },
        {
          "$ref": "#/texts/274"
        },
        {
          "$ref": "#/texts/275"
        }
      ],
      "name": "list",
      "label": "list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_header",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 18.119998931884766,
            "t": 582.3599853515625,
            "r": 35.959999084472656,
            "b": 232.72000122070312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            38
          ]
        }
      ],
      "orig": "arXiv:2412.19505v2 [cs.CV] 30 Dec 2024",
      "text": "arXiv:2412.19505v2 [cs.CV] 30 Dec 2024"
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 58.69683074951172,
            "t": 684.980224609375,
            "r": 553.541259765625,
            "b": 672.1260375976562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            75
          ]
        }
      ],
      "orig": "DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT",
      "text": "DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT",
      "level": 1
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 65.34455108642578,
            "t": 646.935791015625,
            "r": 542.855712890625,
            "b": 620.6676635742188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            126
          ]
        }
      ],
      "orig": "Xiaotao Hu 1 , 2 * Wei Yin 2 * \u2020 Mingkai Jia 1 , 2 Junyuan Deng 1 , 2 Xiaoyang Guo 2 Qian Zhang 2 Xiaoxiao Long 1 \u2021 Ping Tan 1",
      "text": "Xiaotao Hu 1 , 2 * Wei Yin 2 * \u2020 Mingkai Jia 1 , 2 Junyuan Deng 1 , 2 Xiaoyang Guo 2 Qian Zhang 2 Xiaoxiao Long 1 \u2021 Ping Tan 1"
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 171.45809936523438,
            "t": 613.2830810546875,
            "r": 441.0098876953125,
            "b": 589.58935546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            71
          ]
        }
      ],
      "orig": "1 The Hong Kong University of Science and Technology 2 Horizon Robotics",
      "text": "1 The Hong Kong University of Science and Technology 2 Horizon Robotics"
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 190.25001525878906,
            "t": 580.9010620117188,
            "r": 419.0726318359375,
            "b": 570.7032470703125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            32
          ]
        }
      ],
      "orig": "https://tinyurl.com/DrivingWorld",
      "text": "https://tinyurl.com/DrivingWorld"
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 58.60587692260742,
            "t": 405.1277770996094,
            "r": 553.3248291015625,
            "b": 375.0782775878906,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            367
          ]
        }
      ],
      "orig": "Figure 1. Controllable generation results of our method. Our method takes a short video clip as input and can generate multiple possible future driving scenarios conditioned on different pre-defined trajectory paths. We use one straightforward path and one curved path as examples, and the results show that our method achieves accurate control and future prediction.",
      "text": "Figure 1. Controllable generation results of our method. Our method takes a short video clip as input and can generate multiple possible future driving scenarios conditioned on different pre-defined trajectory paths. We use one straightforward path and one curved path as examples, and the results show that our method achieves accurate control and future prediction."
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 160.63189697265625,
            "t": 553.1216430664062,
            "r": 165.26815795898438,
            "b": 516.7583618164062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "Generation #1",
      "text": "Generation #1"
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 75.83734893798828,
            "t": 429.14703369140625,
            "r": 129.41357421875,
            "b": 422.1826171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            16
          ]
        }
      ],
      "orig": "Condition images",
      "text": "Condition images"
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 160.5439910888672,
            "t": 460.7237243652344,
            "r": 165.1802520751953,
            "b": 423.8365783691406,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "Generation #2",
      "text": "Generation #2"
    },
    {
      "self_ref": "#/texts/9",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 477.0839538574219,
            "t": 484.00213623046875,
            "r": 543.1677856445312,
            "b": 477.0300598144531,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            23
          ]
        }
      ],
      "orig": "Condition  trajectories",
      "text": "Condition  trajectories"
    },
    {
      "self_ref": "#/texts/10",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 167.52828979492188,
            "t": 500.55828857421875,
            "r": 172.46905517578125,
            "b": 495.7550354003906,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\ud835\udc65\ud835\udc65",
      "text": "\ud835\udc65\ud835\udc65"
    },
    {
      "self_ref": "#/texts/11",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 150.57447814941406,
            "t": 481.964111328125,
            "r": 155.89833068847656,
            "b": 475.147216796875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\ud835\udc66\ud835\udc66",
      "text": "\ud835\udc66\ud835\udc66"
    },
    {
      "self_ref": "#/texts/12",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 206.96896362304688,
            "t": 509.7457580566406,
            "r": 227.24546813964844,
            "b": 504.2600402832031,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "\ud835\udc61\ud835\udc61 = 5\ud835\udc60\ud835\udc60",
      "text": "\ud835\udc61\ud835\udc61 = 5\ud835\udc60\ud835\udc60"
    },
    {
      "self_ref": "#/texts/13",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 298.08868408203125,
            "t": 509.60015869140625,
            "r": 322.6250915527344,
            "b": 504.3059997558594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "\ud835\udc61\ud835\udc61 = 13\ud835\udc60\ud835\udc60",
      "text": "\ud835\udc61\ud835\udc61 = 13\ud835\udc60\ud835\udc60"
    },
    {
      "self_ref": "#/texts/14",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 392.7787780761719,
            "t": 509.60015869140625,
            "r": 417.31512451171875,
            "b": 504.3059997558594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "\ud835\udc61\ud835\udc61 = 21\ud835\udc60\ud835\udc60",
      "text": "\ud835\udc61\ud835\udc61 = 21\ud835\udc60\ud835\udc60"
    },
    {
      "self_ref": "#/texts/15",
      "parent": {
        "$ref": "#/pictures/0"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 488.5491027832031,
            "t": 509.27069091796875,
            "r": 513.0855102539062,
            "b": 503.9765319824219,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "\ud835\udc61\ud835\udc61 = 28\ud835\udc60\ud835\udc60",
      "text": "\ud835\udc61\ud835\udc61 = 28\ud835\udc60\ud835\udc60"
    },
    {
      "self_ref": "#/texts/16",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 154.49058532714844,
            "t": 360.05810546875,
            "r": 198.85633850097656,
            "b": 351.6535949707031,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "Abstract",
      "text": "Abstract",
      "level": 1
    },
    {
      "self_ref": "#/texts/17",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 57.748023986816406,
            "t": 333.3634338378906,
            "r": 296.199462890625,
            "b": 121.26870727539062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1038
          ]
        }
      ],
      "orig": "Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token",
      "text": "Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token"
    },
    {
      "self_ref": "#/texts/18",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "footnote",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 69.77145385742188,
            "t": 105.84685516357422,
            "r": 137.1487579345703,
            "b": 98.58448791503906,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            22
          ]
        }
      ],
      "orig": "* Contributed equally.",
      "text": "* Contributed equally."
    },
    {
      "self_ref": "#/texts/19",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "footnote",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 69.71167755126953,
            "t": 97.51085662841797,
            "r": 193.99945068359375,
            "b": 88.92748260498047,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            36
          ]
        }
      ],
      "orig": "\u2020 Project leader: yvanwy@outlook.com",
      "text": "\u2020 Project leader: yvanwy@outlook.com"
    },
    {
      "self_ref": "#/texts/20",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "footnote",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 69.70570373535156,
            "t": 87.85385131835938,
            "r": 226.88406372070312,
            "b": 79.27047729492188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            46
          ]
        }
      ],
      "orig": "\u2021 Corresponding author: xxlong@connect.hku.hk.",
      "text": "\u2021 Corresponding author: xxlong@connect.hku.hk."
    },
    {
      "self_ref": "#/texts/21",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 315.766357421875,
            "t": 358.6134338378906,
            "r": 554.9488525390625,
            "b": 218.45892333984375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            683
          ]
        }
      ],
      "orig": "prediction strategy to capture spatial information within each frame. To further enhance generalization ability, we propose a novel masking strategy and reweighting strategy for token prediction to mitigate long-term drifting issues and enable precise control. Our work demonstrates the ability to produce high-fidelity and consistent video clips of over 40 seconds in duration, which is over 2 times longer than state-of-the-art driving world models. Experiments show that, in contrast to prior works, our method achieves superior visual quality and significantly more accurate controllable future video generation. Our code is available at https://github.com/YvanYin/DrivingWorld .",
      "text": "prediction strategy to capture spatial information within each frame. To further enhance generalization ability, we propose a novel masking strategy and reweighting strategy for token prediction to mitigate long-term drifting issues and enable precise control. Our work demonstrates the ability to produce high-fidelity and consistent video clips of over 40 seconds in duration, which is over 2 times longer than state-of-the-art driving world models. Experiments show that, in contrast to prior works, our method achieves superior visual quality and significantly more accurate controllable future video generation. Our code is available at https://github.com/YvanYin/DrivingWorld ."
    },
    {
      "self_ref": "#/texts/22",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 318.027099609375,
            "t": 180.1260528564453,
            "r": 393.8828430175781,
            "b": 171.7095947265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            15
          ]
        }
      ],
      "orig": "1. Introduction",
      "text": "1. Introduction",
      "level": 1
    },
    {
      "self_ref": "#/texts/23",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 317.11376953125,
            "t": 159.53546142578125,
            "r": 554.5421752929688,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            414
          ]
        }
      ],
      "orig": "In recent years, autoregressive (AR) learning schemes have achieved significant success in natural language processing, as demonstrated by models like the GPT series [3 , 28 , 29]. These models predict future text responses from past data, making AR approaches the leading candidates in the pursuit of Artificial General Intelligence (AGI). Inspired by these advancements, many researchers have sought to replicate",
      "text": "In recent years, autoregressive (AR) learning schemes have achieved significant success in natural language processing, as demonstrated by models like the GPT series [3 , 28 , 29]. These models predict future text responses from past data, making AR approaches the leading candidates in the pursuit of Artificial General Intelligence (AGI). Inspired by these advancements, many researchers have sought to replicate"
    },
    {
      "self_ref": "#/texts/24",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 304.6148376464844,
            "t": 57.846717834472656,
            "r": 307.43426513671875,
            "b": 51.11199951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/25",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 58.35021209716797,
            "t": 716.8414306640625,
            "r": 294.661376953125,
            "b": 695.9201049804688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            101
          ]
        }
      ],
      "orig": "this success in visual tasks, such as building vision-based world models for autonomous driving [17].",
      "text": "this success in visual tasks, such as building vision-based world models for autonomous driving [17]."
    },
    {
      "self_ref": "#/texts/26",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 58.5488166809082,
            "t": 691.0894165039062,
            "r": 295.936767578125,
            "b": 526.7061157226562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            790
          ]
        }
      ],
      "orig": "A critical capability in autonomous driving systems is future event prediction [10]. However, many prediction models rely heavily on large volumes of labeled data, making them vulnerable to out-of-distribution and long-tail scenarios [27 , 30 , 37]. This is especially problematic for rare and extreme cases, such as accidents, where obtaining sufficient training data is challenging. A promising solution lies in autoregressive world models, which learn comprehensive information from unlabeled data like massive videos through unsupervised learning. This enables more robust decisionmaking in driving scenarios. These world models have the potential to reason under uncertainty and reduce catastrophic errors, thereby improving the generalization and safety of autonomous driving systems.",
      "text": "A critical capability in autonomous driving systems is future event prediction [10]. However, many prediction models rely heavily on large volumes of labeled data, making them vulnerable to out-of-distribution and long-tail scenarios [27 , 30 , 37]. This is especially problematic for rare and extreme cases, such as accidents, where obtaining sufficient training data is challenging. A promising solution lies in autoregressive world models, which learn comprehensive information from unlabeled data like massive videos through unsupervised learning. This enables more robust decisionmaking in driving scenarios. These world models have the potential to reason under uncertainty and reduce catastrophic errors, thereby improving the generalization and safety of autonomous driving systems."
    },
    {
      "self_ref": "#/texts/27",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 58.530487060546875,
            "t": 521.8744506835938,
            "r": 295.93487548828125,
            "b": 369.44610595703125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            758
          ]
        }
      ],
      "orig": "The prior work, GAIA-1 [17], is the first to extend the GPT framework from language to video, aiming to develop a video-based world model. Similar to natural language processing, GAIA transforms 4D temporally correlated frames into a sequence of 1D feature tokens and employs the nexttoken prediction strategy to generate future video clips. However, the classic GPT framework, primarily designed for handling 1D contextual information, lacks the inherent capability to effectively model the spatial and temporal dynamics necessary for video generation. As a result, videos produced by GAIA-1 often suffer from low quality and noticeable artifacts, highlighting the challenge of achieving fidelity and coherence within a GPT-style video generation framework.",
      "text": "The prior work, GAIA-1 [17], is the first to extend the GPT framework from language to video, aiming to develop a video-based world model. Similar to natural language processing, GAIA transforms 4D temporally correlated frames into a sequence of 1D feature tokens and employs the nexttoken prediction strategy to generate future video clips. However, the classic GPT framework, primarily designed for handling 1D contextual information, lacks the inherent capability to effectively model the spatial and temporal dynamics necessary for video generation. As a result, videos produced by GAIA-1 often suffer from low quality and noticeable artifacts, highlighting the challenge of achieving fidelity and coherence within a GPT-style video generation framework."
    },
    {
      "self_ref": "#/texts/28",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 58.293426513671875,
            "t": 364.6154479980469,
            "r": 295.93701171875,
            "b": 140.45611572265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1097
          ]
        }
      ],
      "orig": "In this paper, we introduce DrivingWorld, a driving world model built on a GPT-style video generation framework. Our primary goal is to enhance the modeling of temporal coherence in an autoregressive framework to create more accurate and reliable world models. To achieve this, our model incorporates three key innovations: 1) Temporal-Aware Tokenization: We propose a temporal-aware tokenizer that transforms video frames into temporally coherent tokens, reformulating the task of future video prediction as predicting future tokens in the sequence. 2) Hybrid Token Prediction: Instead of relying solely on the next-token prediction strategy, we introduce a next-state prediction strategy to better model temporal coherence between consecutive states. Afterward, the next-token prediction strategy is applied to capture spatial information within each state. 3) Long-time Controllable Strategies: To improve robustness, we implement random token dropout and balanced attention strategies during autoregressive training, enabling the generation of longer-duration videos with more precise control.",
      "text": "In this paper, we introduce DrivingWorld, a driving world model built on a GPT-style video generation framework. Our primary goal is to enhance the modeling of temporal coherence in an autoregressive framework to create more accurate and reliable world models. To achieve this, our model incorporates three key innovations: 1) Temporal-Aware Tokenization: We propose a temporal-aware tokenizer that transforms video frames into temporally coherent tokens, reformulating the task of future video prediction as predicting future tokens in the sequence. 2) Hybrid Token Prediction: Instead of relying solely on the next-token prediction strategy, we introduce a next-state prediction strategy to better model temporal coherence between consecutive states. Afterward, the next-token prediction strategy is applied to capture spatial information within each state. 3) Long-time Controllable Strategies: To improve robustness, we implement random token dropout and balanced attention strategies during autoregressive training, enabling the generation of longer-duration videos with more precise control."
    },
    {
      "self_ref": "#/texts/29",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 58.550411224365234,
            "t": 135.62545776367188,
            "r": 295.9113464355469,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            280
          ]
        }
      ],
      "orig": "Overall, our work enhances temporal coherence in video generation using the AR framework, learning meaningful representations of future evolution. Experiments show that the proposed model achieves good generalization performance, is capable of generating over 40 seconds video se-",
      "text": "Overall, our work enhances temporal coherence in video generation using the AR framework, learning meaningful representations of future evolution. Experiments show that the proposed model achieves good generalization performance, is capable of generating over 40 seconds video se-"
    },
    {
      "self_ref": "#/texts/30",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 303.8078918457031,
            "t": 57.846717834472656,
            "r": 308.2412414550781,
            "b": 51.11199951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "2",
      "text": "2"
    },
    {
      "self_ref": "#/texts/31",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 317.3795166015625,
            "t": 716.8414306640625,
            "r": 554.6624145507812,
            "b": 695.9201049804688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            115
          ]
        }
      ],
      "orig": "quences, and provides accurate next-step trajectory predictions, maintaining a reasonable level of controllability.",
      "text": "quences, and provides accurate next-step trajectory predictions, maintaining a reasonable level of controllability."
    },
    {
      "self_ref": "#/texts/32",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 317.4532470703125,
            "t": 682.2281494140625,
            "r": 400.2071533203125,
            "b": 673.8356323242188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            15
          ]
        }
      ],
      "orig": "2. Related Work",
      "text": "2. Related Work",
      "level": 1
    },
    {
      "self_ref": "#/texts/33",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 316.9377136230469,
            "t": 661.8214721679688,
            "r": 554.6884155273438,
            "b": 509.403076171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            729
          ]
        }
      ],
      "orig": "World Model. The world model [24] captures a comprehensive representation of the environment and forecasts future states based on a sequence of actions. World models have been extensively explored in both game [11-13] and laboratory environments [40]. Dreamer [11] trained a latent dynamics model using past experiences to forecast state values and actions within a latent space. DreamerV2 [12] built upon the original Dreamer model, reaching human-level performance in Atari games. DreamerV3 [13] employed larger networks and successfully learned to acquire diamonds in Minecraft from scratch. DayDreamer [40] extended Dreamer [11] to train four robots in the real world, successfully tackling locomotion and manipulation tasks.",
      "text": "World Model. The world model [24] captures a comprehensive representation of the environment and forecasts future states based on a sequence of actions. World models have been extensively explored in both game [11-13] and laboratory environments [40]. Dreamer [11] trained a latent dynamics model using past experiences to forecast state values and actions within a latent space. DreamerV2 [12] built upon the original Dreamer model, reaching human-level performance in Atari games. DreamerV3 [13] employed larger networks and successfully learned to acquire diamonds in Minecraft from scratch. DayDreamer [40] extended Dreamer [11] to train four robots in the real world, successfully tackling locomotion and manipulation tasks."
    },
    {
      "self_ref": "#/texts/34",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 317.28045654296875,
            "t": 506.34844970703125,
            "r": 554.6871337890625,
            "b": 377.8301086425781,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            608
          ]
        }
      ],
      "orig": "Recently, world models for driving scenarios have garnered significant attention in both academia and industry. Most previous works [1 , 6 , 7 , 15] have been limited to simulators or well-controlled lab environments. Drive-WM [39] explored real-world driving planners using diffusion models. GAIA-1 [17] investigated real-world driving planners based on autoregressive models, but GAIA-1 had large parameters and computational demands, which increased as the number of condition frames grew. In this paper, we propose an efficient world model in an autoregressive framework for autonomous driving scenarios.",
      "text": "Recently, world models for driving scenarios have garnered significant attention in both academia and industry. Most previous works [1 , 6 , 7 , 15] have been limited to simulators or well-controlled lab environments. Drive-WM [39] explored real-world driving planners using diffusion models. GAIA-1 [17] investigated real-world driving planners based on autoregressive models, but GAIA-1 had large parameters and computational demands, which increased as the number of condition frames grew. In this paper, we propose an efficient world model in an autoregressive framework for autonomous driving scenarios."
    },
    {
      "self_ref": "#/texts/35",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 317.0535888671875,
            "t": 374.8641662597656,
            "r": 554.6848754882812,
            "b": 150.6350860595703,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1063
          ]
        }
      ],
      "orig": "VQVAE. VQVAE [36] learned a discrete codebook representation via vector quantization to model image distributions. VQGAN [8] improved realism by incorporating LPIPS loss [46] and adversarial PatchGAN loss [18]. MoVQ [47] tackled VQGAN's spatially conditional normalization issue by embedding spatially variant information into quantized vectors. LlamaGen [33] further fine-tuned VQGAN, showing that a smaller codebook vector dimension and a larger codebook size could enhance reconstruction performance. While VQGAN-based structures were widely used, some methods explored more efficient architectures. ViTVQGAN [43] replaced the convolutional encoder-decoder with a vision transformer, improving the model's ability to capture long-range dependencies. VAR [34] employed a multi-scale structure to predict subsequent scales from previous ones, enhancing generation quality and speed. However, these methods focused on single-image processing, preventing them from capturing temporal consistency. To address this, we propose a temporal-aware tokenizer and decoder.",
      "text": "VQVAE. VQVAE [36] learned a discrete codebook representation via vector quantization to model image distributions. VQGAN [8] improved realism by incorporating LPIPS loss [46] and adversarial PatchGAN loss [18]. MoVQ [47] tackled VQGAN's spatially conditional normalization issue by embedding spatially variant information into quantized vectors. LlamaGen [33] further fine-tuned VQGAN, showing that a smaller codebook vector dimension and a larger codebook size could enhance reconstruction performance. While VQGAN-based structures were widely used, some methods explored more efficient architectures. ViTVQGAN [43] replaced the convolutional encoder-decoder with a vision transformer, improving the model's ability to capture long-range dependencies. VAR [34] employed a multi-scale structure to predict subsequent scales from previous ones, enhancing generation quality and speed. However, these methods focused on single-image processing, preventing them from capturing temporal consistency. To address this, we propose a temporal-aware tokenizer and decoder."
    },
    {
      "self_ref": "#/texts/36",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 317.0535888671875,
            "t": 147.66015625,
            "r": 553.4689331054688,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            347
          ]
        }
      ],
      "orig": "Video Generation. Currently, there are three mainstream video generation models: GAN-based, diffusion-based, and GPT-based methods. GAN-based methods [31 , 35 , 44] often face several challenges, such as mode collapse, where the diversity of the videos generated by the generator becomes limited. Additionally, the adversarial learning between the",
      "text": "Video Generation. Currently, there are three mainstream video generation models: GAN-based, diffusion-based, and GPT-based methods. GAN-based methods [31 , 35 , 44] often face several challenges, such as mode collapse, where the diversity of the videos generated by the generator becomes limited. Additionally, the adversarial learning between the"
    },
    {
      "self_ref": "#/texts/37",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 58.59145736694336,
            "t": 716.8414306640625,
            "r": 295.93487548828125,
            "b": 564.4131469726562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            754
          ]
        }
      ],
      "orig": "generator and discriminator can lead to instability during training. One major issue with diffusion-based methods is their inability to generate precisely controlled videos. The stochastic nature of the diffusion process introduces randomness at each step, making it difficult to enforce strict control over specific attributes in the generated content. On the other hand, traditional GPT-based methods [14 , 41] allow for a certain level of control, but their computational cost grows quadratically with the sequence length, significantly impacting model efficiency. In this paper, we propose a decoupled spatio-temporal world model framework, which ensures precise control while significantly reducing computational cost and improving model efficiency.",
      "text": "generator and discriminator can lead to instability during training. One major issue with diffusion-based methods is their inability to generate precisely controlled videos. The stochastic nature of the diffusion process introduces randomness at each step, making it difficult to enforce strict control over specific attributes in the generated content. On the other hand, traditional GPT-based methods [14 , 41] allow for a certain level of control, but their computational cost grows quadratically with the sequence length, significantly impacting model efficiency. In this paper, we propose a decoupled spatio-temporal world model framework, which ensures precise control while significantly reducing computational cost and improving model efficiency."
    },
    {
      "self_ref": "#/texts/38",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 58.6912841796875,
            "t": 547.5411987304688,
            "r": 110.03886413574219,
            "b": 539.16064453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "3. Method",
      "text": "3. Method",
      "level": 1
    },
    {
      "self_ref": "#/texts/39",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 58.35697937011719,
            "t": 526.074462890625,
            "r": 295.9282531738281,
            "b": 398.32421875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            641
          ]
        }
      ],
      "orig": "Our proposed world model, DrivingWorld, leverages a GPTstyle architecture to predict future states with high efficiency, capable of extending predictions beyond 40 seconds at 10Hz. This model is designed to comprehend past real-world states and forecast future video content and vehicle motions. DrivingWorld is specifically focused on predicting the next state at time T + 1 based on the historical states from time 1 to T , and we can generate long videos by sequentially predicting future states one by one. Each state at time t is represented as \\left [ theta _t, (x_t, y_t), \\mathbf is its location, and \\mathbf {I}_t  is the current fr",
      "text": "Our proposed world model, DrivingWorld, leverages a GPTstyle architecture to predict future states with high efficiency, capable of extending predictions beyond 40 seconds at 10Hz. This model is designed to comprehend past real-world states and forecast future video content and vehicle motions. DrivingWorld is specifically focused on predicting the next state at time T + 1 based on the historical states from time 1 to T , and we can generate long videos by sequentially predicting future states one by one. Each state at time t is represented as \\left [ theta _t, (x_t, y_t), \\mathbf is its location, and \\mathbf {I}_t  is the current fr"
    },
    {
      "self_ref": "#/texts/40",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 58.289390563964844,
            "t": 393.4514465332031,
            "r": 295.9348449707031,
            "b": 215.81707763671875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            818
          ]
        }
      ],
      "orig": "As shown in Figure 2, our proposed DrivingWorld not only generates future states based on past observations {\\{[ \\thet supports controllable simulation of complex driving scenarios by manipulating the vehicle's location and orientation. Section 3.1 details our proposed tokenizers for encoding temporal multimodal information into the unified latent space. To model the relationships between long-term sequential states, we introduce a GPT-style temporal multimodel decoupled world model in Section 3.2. To extract the state from the tokens predicted by the world model, we also introduce a temporal decoder, which is discussed in detail in Section 3.3 . Additionally, we introduce long-term controllable strategies in Section 3.4 to address the drifting problem and enhance the robustness of the proposed world model.",
      "text": "As shown in Figure 2, our proposed DrivingWorld not only generates future states based on past observations {\\{[ \\thet supports controllable simulation of complex driving scenarios by manipulating the vehicle's location and orientation. Section 3.1 details our proposed tokenizers for encoding temporal multimodal information into the unified latent space. To model the relationships between long-term sequential states, we introduce a GPT-style temporal multimodel decoupled world model in Section 3.2. To extract the state from the tokens predicted by the world model, we also introduce a temporal decoder, which is discussed in detail in Section 3.3 . Additionally, we introduce long-term controllable strategies in Section 3.4 to address the drifting problem and enhance the robustness of the proposed world model."
    },
    {
      "self_ref": "#/texts/41",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 58.67534255981445,
            "t": 202.2666015625,
            "r": 123.93560028076172,
            "b": 194.55152893066406,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "3.1. Tokenizer",
      "text": "3.1. Tokenizer",
      "level": 1
    },
    {
      "self_ref": "#/texts/42",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 58.359012603759766,
            "t": 183.45542907714844,
            "r": 295.91131591796875,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            524
          ]
        }
      ],
      "orig": "Tokenization [36 , 47] converts continuous data into discrete tokens, enabling integration with language models and enhanced multimodal sequence modeling. In our approach, the tokenizer maps multimodal states into a unified discrete space, which enables accurate and controllable multimodal generation. To produce temporally consistent embeddings for images, we propose a temporal-aware vector quantized tokenizer. Our proposed vehicle pose tokenizer discretizes pose trajectories and integrates them into our DrivingWorld .",
      "text": "Tokenization [36 , 47] converts continuous data into discrete tokens, enabling integration with language models and enhanced multimodal sequence modeling. In our approach, the tokenizer maps multimodal states into a unified discrete space, which enables accurate and controllable multimodal generation. To produce temporally consistent embeddings for images, we propose a temporal-aware vector quantized tokenizer. Our proposed vehicle pose tokenizer discretizes pose trajectories and integrates them into our DrivingWorld ."
    },
    {
      "self_ref": "#/texts/43",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 303.93743896484375,
            "t": 57.846717834472656,
            "r": 307.8128662109375,
            "b": 50.982486724853516,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "3",
      "text": "3"
    },
    {
      "self_ref": "#/texts/44",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 317.41259765625,
            "t": 716.9310913085938,
            "r": 554.8341064453125,
            "b": 707.99462890625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            53
          ]
        }
      ],
      "orig": "Prelimilary: Single Image Vector Quantized Tokenizer.",
      "text": "Prelimilary: Single Image Vector Quantized Tokenizer.",
      "level": 1
    },
    {
      "self_ref": "#/texts/45",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 317.11376953125,
            "t": 704.8864135742188,
            "r": 554.669677734375,
            "b": 624.1891479492188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            379
          ]
        }
      ],
      "orig": "The single image vector quantized (VQ) tokenizer, as described in [36], is designed to convert an image feature map \\mathbf {f} \\in \\mathbb {R H\u00d7W . The quantizer utilizes a learned discrete codebook \\mathcal {Z} \\in \\mathbb {R}^{K\\times C} , co (i,j)to the index q (i,j) of the closest code in Z. This method enables the conversion of continuous image data into discrete tokens.",
      "text": "The single image vector quantized (VQ) tokenizer, as described in [36], is designed to convert an image feature map \\mathbf {f} \\in \\mathbb {R H\u00d7W . The quantizer utilizes a learned discrete codebook \\mathcal {Z} \\in \\mathbb {R}^{K\\times C} , co (i,j)to the index q (i,j) of the closest code in Z. This method enables the conversion of continuous image data into discrete tokens."
    },
    {
      "self_ref": "#/texts/46",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 317.2359924316406,
            "t": 620.8831787109375,
            "r": 554.6810302734375,
            "b": 542.1683959960938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            328
          ]
        }
      ],
      "orig": "Temporal-aware Vector Quantized Tokenizer. Singleimage VQ tokenizers often struggle to produce temporally consistent embeddings, causing discontinuous video predictions and hindering the training of the world model. The image sequence  \\{\\mathbf {I}_t}\\}^T_{t=1} feature is processed independently, lacking temporal information.",
      "text": "Temporal-aware Vector Quantized Tokenizer. Singleimage VQ tokenizers often struggle to produce temporally consistent embeddings, causing discontinuous video predictions and hindering the training of the world model. The image sequence  \\{\\mathbf {I}_t}\\}^T_{t=1} feature is processed independently, lacking temporal information."
    },
    {
      "self_ref": "#/texts/47",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 317.0472106933594,
            "t": 536.720458984375,
            "r": 554.681396484375,
            "b": 360.06231689453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            848
          ]
        }
      ],
      "orig": "To address this issue, we propose a temporal-aware vector quantized tokenizer designed to ensure consistent embeddings over time. Specifically, to capture temporal dependencies, we insert a self-attention layer both before and after VQGAN [8] quantization, where the attention operates along the temporal dimension. This allows our model to capture long-term temporal relationships between frames, improving coherence and consistency in the generated sequences. Our model builds upon the open-source VQGAN [8] implementation from LlammaGen [33]. The integration of our straightforward yet effective temporal self-attentions can be seamlessly incorporated into the original framework, followed by fine-tuning to develop a robust and generalizable temporal-aware VQ tokenizer. \\{\\mathbf {f}_{t}\\}^T self-attention H(\u00b7) before performing quantization:",
      "text": "To address this issue, we propose a temporal-aware vector quantized tokenizer designed to ensure consistent embeddings over time. Specifically, to capture temporal dependencies, we insert a self-attention layer both before and after VQGAN [8] quantization, where the attention operates along the temporal dimension. This allows our model to capture long-term temporal relationships between frames, improving coherence and consistency in the generated sequences. Our model builds upon the open-source VQGAN [8] implementation from LlammaGen [33]. The integration of our straightforward yet effective temporal self-attentions can be seamlessly incorporated into the original framework, followed by fine-tuning to develop a robust and generalizable temporal-aware VQ tokenizer. \\{\\mathbf {f}_{t}\\}^T self-attention H(\u00b7) before performing quantization:"
    },
    {
      "self_ref": "#/texts/48",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 322.2536926269531,
            "t": 343.1208801269531,
            "r": 322.3481750488281,
            "b": 337.35546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\\s",
      "text": "\\s"
    },
    {
      "self_ref": "#/texts/49",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 317.0535888671875,
            "t": 318.2189636230469,
            "r": 554.6815185546875,
            "b": 176.68112182617188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            689
          ]
        }
      ],
      "orig": "where lookup(Z, k) denotes the k-th vector in codebook Z . Vehicle Pose Tokenizer. To accurately represent the vehicle's ego status, including its orientation \\theta  and (x, y) , we adopt a coordinate system centered at the ego vehicle, as depicted in Figure 2. Instead of global p adopt the relative poses between adjacent time steps. This is because global poses present a significant challenge due to the increasing magnitude of absolute pose values over longterm sequences. This growth makes normalization difficult and reduces model robustness. As sequences grow longer, managing these large pose values becomes increasingly difficult, hindering effective long-term video generation.",
      "text": "where lookup(Z, k) denotes the k-th vector in codebook Z . Vehicle Pose Tokenizer. To accurately represent the vehicle's ego status, including its orientation \\theta  and (x, y) , we adopt a coordinate system centered at the ego vehicle, as depicted in Figure 2. Instead of global p adopt the relative poses between adjacent time steps. This is because global poses present a significant challenge due to the increasing magnitude of absolute pose values over longterm sequences. This growth makes normalization difficult and reduces model robustness. As sequences grow longer, managing these large pose values becomes increasingly difficult, hindering effective long-term video generation."
    },
    {
      "self_ref": "#/texts/50",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 317.2804870605469,
            "t": 176.21429443359375,
            "r": 554.6743774414062,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            486
          ]
        }
      ],
      "orig": "For the sequence of the vehicle's orientation {\u03b8t} T t=1 and location {(xt, yt)} T t=1 , we propose to compute relative values for each time step with respect to the previous one. The relative location and orientation at the first time step is initialized as zero. The ego-centric status sequence is given by {\u2206\u03b8t} T t=1 and {(\u2206xt , \u2206yt)} T t=1 . To tokenize them, we discretize the ego's surrounding space. Specifically, we discretize the orientation into \u03b1 categories, and the X and Y",
      "text": "For the sequence of the vehicle's orientation {\u03b8t} T t=1 and location {(xt, yt)} T t=1 , we propose to compute relative values for each time step with respect to the previous one. The relative location and orientation at the first time step is initialized as zero. The ego-centric status sequence is given by {\u2206\u03b8t} T t=1 and {(\u2206xt , \u2206yt)} T t=1 . To tokenize them, we discretize the ego's surrounding space. Specifically, we discretize the orientation into \u03b1 categories, and the X and Y"
    },
    {
      "self_ref": "#/texts/51",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.60974884033203,
            "t": 588.255859375,
            "r": 554.1309814453125,
            "b": 545.5833129882812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            535
          ]
        }
      ],
      "orig": "Figure 2. Pipeline of DrivingWorld. The vehicle orientations {\u03b8t} T t=1 , ego locations {(xt, yt)} T t=1 , and a front-view image sequence {It} T t=1 are taken as the conditional input, which are first tokenized as latent embeddings. Then our proposed multi-modal world model attempts to comprehend them and forecast the future states, which are detokenized to the vehicle orientation \u02c6 \u03b8 T +1 , location (\u02c6xxT +1 , y \u02c6 T +1 ) , and the front-view image \u02c6 IT +1. With the autoregressive process, we can generate over 40 seconds videos.",
      "text": "Figure 2. Pipeline of DrivingWorld. The vehicle orientations {\u03b8t} T t=1 , ego locations {(xt, yt)} T t=1 , and a front-view image sequence {It} T t=1 are taken as the conditional input, which are first tokenized as latent embeddings. Then our proposed multi-modal world model attempts to comprehend them and forecast the future states, which are detokenized to the vehicle orientation \u02c6 \u03b8 T +1 , location (\u02c6xxT +1 , y \u02c6 T +1 ) , and the front-view image \u02c6 IT +1. With the autoregressive process, we can generate over 40 seconds videos."
    },
    {
      "self_ref": "#/texts/52",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 159.88072204589844,
            "t": 712.4813842773438,
            "r": 195.69480895996094,
            "b": 705.9903564453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            11
          ]
        }
      ],
      "orig": "ToTokenizer",
      "text": "ToTokenizer"
    },
    {
      "self_ref": "#/texts/53",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 287.1357727050781,
            "t": 712.1041259765625,
            "r": 335.11492919921875,
            "b": 705.60400390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "WoWorld Model",
      "text": "WoWorld Model"
    },
    {
      "self_ref": "#/texts/54",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 162.01622009277344,
            "t": 642.3894653320312,
            "r": 192.7741241455078,
            "b": 635.4713745117188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            11
          ]
        }
      ],
      "orig": "TeTemporal-",
      "text": "TeTemporal-"
    },
    {
      "self_ref": "#/texts/55",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 162.3915557861328,
            "t": 633.2081298828125,
            "r": 192.4828338623047,
            "b": 626.5541381835938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "aware  VQ",
      "text": "aware  VQ"
    },
    {
      "self_ref": "#/texts/56",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 162.64688110351562,
            "t": 624.2833251953125,
            "r": 192.49192810058594,
            "b": 618.8740844726562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            11
          ]
        }
      ],
      "orig": "ToTokenizer",
      "text": "ToTokenizer"
    },
    {
      "self_ref": "#/texts/57",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 278.533203125,
            "t": 690.6725463867188,
            "r": 344.18316650390625,
            "b": 683.7544555664062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "TeTemporal-multimodal",
      "text": "TeTemporal-multimodal"
    },
    {
      "self_ref": "#/texts/58",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 288.5297546386719,
            "t": 681.6194458007812,
            "r": 334.081787109375,
            "b": 676.210205078125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "Fusion Module",
      "text": "Fusion Module"
    },
    {
      "self_ref": "#/texts/59",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 291.65069580078125,
            "t": 637.8629760742188,
            "r": 331.0013732910156,
            "b": 632.4537353515625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "Internal-state",
      "text": "Internal-state"
    },
    {
      "self_ref": "#/texts/60",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 275.903564453125,
            "t": 628.8099365234375,
            "r": 346.61553955078125,
            "b": 621.8767700195312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "Autoregressive Module",
      "text": "Autoregressive Module"
    },
    {
      "self_ref": "#/texts/61",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 413.6521301269531,
            "t": 695.1990966796875,
            "r": 435.9704284667969,
            "b": 689.7823486328125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "VeVehicle",
      "text": "VeVehicle"
    },
    {
      "self_ref": "#/texts/62",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 417.8971862792969,
            "t": 685.904541015625,
            "r": 431.8087158203125,
            "b": 680.7367553710938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            4
          ]
        }
      ],
      "orig": "Pose",
      "text": "Pose"
    },
    {
      "self_ref": "#/texts/63",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 412.2507629394531,
            "t": 677.0929565429688,
            "r": 437.6974182128906,
            "b": 671.6837158203125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "Decoder",
      "text": "Decoder"
    },
    {
      "self_ref": "#/texts/64",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 409.1585388183594,
            "t": 712.1041259765625,
            "r": 439.6945495605469,
            "b": 705.6130981445312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "Decoder",
      "text": "Decoder"
    },
    {
      "self_ref": "#/texts/65",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 409.46673583984375,
            "t": 642.3894653320312,
            "r": 440.22467041015625,
            "b": 635.4713745117188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            11
          ]
        }
      ],
      "orig": "TeTemporal-",
      "text": "TeTemporal-"
    },
    {
      "self_ref": "#/texts/66",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 416.166259765625,
            "t": 631.5559692382812,
            "r": 433.6763916015625,
            "b": 627.9271850585938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            5
          ]
        }
      ],
      "orig": "aware",
      "text": "aware"
    },
    {
      "self_ref": "#/texts/67",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 105.05779266357422,
            "t": 621.401123046875,
            "r": 111.32656860351562,
            "b": 616.5996704101562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/68",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 412.2507629394531,
            "t": 624.2833251953125,
            "r": 437.6974182128906,
            "b": 618.8740844726562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "Decoder",
      "text": "Decoder"
    },
    {
      "self_ref": "#/texts/69",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 220.51101684570312,
            "t": 630.2220458984375,
            "r": 221.15077209472656,
            "b": 625.5989990234375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/70",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 230.50714111328125,
            "t": 630.033447265625,
            "r": 231.1468963623047,
            "b": 625.410400390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/71",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 249.74485778808594,
            "t": 630.033447265625,
            "r": 250.38461303710938,
            "b": 625.410400390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/72",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 238.126708984375,
            "t": 692.0274658203125,
            "r": 242.7498016357422,
            "b": 691.3877563476562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/73",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 238.126708984375,
            "t": 673.5440673828125,
            "r": 242.7498016357422,
            "b": 672.9043579101562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/74",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 238.126708984375,
            "t": 652.4202880859375,
            "r": 242.7498016357422,
            "b": 651.7805786132812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/75",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 238.126708984375,
            "t": 615.4535522460938,
            "r": 242.7498016357422,
            "b": 614.8138427734375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/76",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 238.45816040039062,
            "t": 629.47509765625,
            "r": 242.17955017089844,
            "b": 625.7537231445312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/77",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 237.24932861328125,
            "t": 707.5496826171875,
            "r": 243.6309814453125,
            "b": 706.6451416015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/78",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 104.77131652832031,
            "t": 691.5249633789062,
            "r": 110.81858825683594,
            "b": 685.4777221679688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/79",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 129.8211212158203,
            "t": 653.7337646484375,
            "r": 133.0462646484375,
            "b": 650.657958984375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "x",
      "text": "x"
    },
    {
      "self_ref": "#/texts/80",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 112.04466247558594,
            "t": 662.0324096679688,
            "r": 115.35808563232422,
            "b": 657.496826171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "y",
      "text": "y"
    },
    {
      "self_ref": "#/texts/81",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 166.20162963867188,
            "t": 695.1990966796875,
            "r": 188.51991271972656,
            "b": 689.7823486328125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "VeVehicle",
      "text": "VeVehicle"
    },
    {
      "self_ref": "#/texts/82",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 170.44664001464844,
            "t": 685.904541015625,
            "r": 184.35818481445312,
            "b": 680.7367553710938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            4
          ]
        }
      ],
      "orig": "Pose",
      "text": "Pose"
    },
    {
      "self_ref": "#/texts/83",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 162.64688110351562,
            "t": 677.0929565429688,
            "r": 192.49192810058594,
            "b": 671.6837158203125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            11
          ]
        }
      ],
      "orig": "ToTokenizer",
      "text": "ToTokenizer"
    },
    {
      "self_ref": "#/texts/84",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 238.126708984375,
            "t": 641.1038818359375,
            "r": 242.7498016357422,
            "b": 640.4641723632812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/85",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 371.77276611328125,
            "t": 630.2220458984375,
            "r": 372.4125061035156,
            "b": 625.5989990234375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/86",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 491.7120666503906,
            "t": 689.639892578125,
            "r": 497.75933837890625,
            "b": 683.5926513671875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/87",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 517.5941162109375,
            "t": 641.2857666015625,
            "r": 520.8192749023438,
            "b": 638.2099609375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "x",
      "text": "x"
    },
    {
      "self_ref": "#/texts/88",
      "parent": {
        "$ref": "#/pictures/1"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 502.2695617675781,
            "t": 652.2249145507812,
            "r": 505.5829772949219,
            "b": 647.6893310546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "y",
      "text": "y"
    },
    {
      "self_ref": "#/texts/89",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.86861801147461,
            "t": 532.7255859375,
            "r": 294.55255126953125,
            "b": 513.6074829101562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            96
          ]
        }
      ],
      "orig": "axes into \u03b2 and \u03b3 bins, respectively. Thus, the relative pose at time t is tokenized as follows:",
      "text": "axes into \u03b2 and \u03b3 bins, respectively. Thus, the relative pose at time t is tokenized as follows:"
    },
    {
      "self_ref": "#/texts/90",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 284.2792053222656,
            "t": 482.792724609375,
            "r": 294.9391784667969,
            "b": 474.3045959472656,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "(2)",
      "text": "(2)"
    },
    {
      "self_ref": "#/texts/91",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.62194061279297,
            "t": 443.4964294433594,
            "r": 295.91143798828125,
            "b": 408.497314453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            178
          ]
        }
      ],
      "orig": "Finally, we process the past T real-world sta { \\{\\left [\\theta _t, (x_t, y_t), \\mathbf {I_t \\right ] \\}}_{t=1}^{T} and tokenize them into a discrete sequence  {\\{ \\left [\\phi _{",
      "text": "Finally, we process the past T real-world sta { \\{\\left [\\theta _t, (x_t, y_t), \\mathbf {I_t \\right ] \\}}_{t=1}^{T} and tokenize them into a discrete sequence  {\\{ \\left [\\phi _{"
    },
    {
      "self_ref": "#/texts/92",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.67534255981445,
            "t": 387.3617248535156,
            "r": 139.61778259277344,
            "b": 379.6685791015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            16
          ]
        }
      ],
      "orig": "3.2. World Model",
      "text": "3.2. World Model",
      "level": 1
    },
    {
      "self_ref": "#/texts/93",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.35021209716797,
            "t": 369.6904296875,
            "r": 295.9143981933594,
            "b": 190.7880859375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            838
          ]
        }
      ],
      "orig": "The world model aims to comprehend past state inputs, mimic real-world dynamics, and predict future states. In our context, it forecasts upcoming driving scenarios and plans a feasible future trajectory. To do this, the world model concatenates historical state tokens {\\{ \\l long sequence, where the 2D image tokens are unfolded into a 1D form in zig-zag order. Thus the objective is to predict the next state r T +1 = (\u03d5T +1, vT +1 , q 1 T +1, . . . , q H\u00d7W T +1 ) based on the sequence of past observations {rt} T t=1 , capturing both temporal and multimodal dependencies. Note that all discrete tokens from different modalities are mapped into a shared latent space by their respective learnable embedding layers before being fed to the world model, i.e. h t = Emb(rt). All subsequent processes are conducted within this latent space.",
      "text": "The world model aims to comprehend past state inputs, mimic real-world dynamics, and predict future states. In our context, it forecasts upcoming driving scenarios and plans a feasible future trajectory. To do this, the world model concatenates historical state tokens {\\{ \\l long sequence, where the 2D image tokens are unfolded into a 1D form in zig-zag order. Thus the objective is to predict the next state r T +1 = (\u03d5T +1, vT +1 , q 1 T +1, . . . , q H\u00d7W T +1 ) based on the sequence of past observations {rt} T t=1 , capturing both temporal and multimodal dependencies. Note that all discrete tokens from different modalities are mapped into a shared latent space by their respective learnable embedding layers before being fed to the world model, i.e. h t = Emb(rt). All subsequent processes are conducted within this latent space."
    },
    {
      "self_ref": "#/texts/94",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.5488166809082,
            "t": 187.86915588378906,
            "r": 295.9334716796875,
            "b": 132.34706115722656,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            278
          ]
        }
      ],
      "orig": "Prelimilary: Next-Token Prediction. A straightforward method is to use the GPT-2 [29] structure for 1D sequential next-token prediction. Figure 3 (a) shows a simplified example. The causal attention is applied for next-token prediction and the i-th token in T + 1 is modeled as:",
      "text": "Prelimilary: Next-Token Prediction. A straightforward method is to use the GPT-2 [29] structure for 1D sequential next-token prediction. Figure 3 (a) shows a simplified example. The causal attention is applied for next-token prediction and the i-th token in T + 1 is modeled as:"
    },
    {
      "self_ref": "#/texts/95",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 84.3586654663086,
            "t": 120.11204528808594,
            "r": 84.41680145263672,
            "b": 118.57780456542969,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\\h",
      "text": "\\h"
    },
    {
      "self_ref": "#/texts/96",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 58.35439682006836,
            "t": 100.42694854736328,
            "r": 294.7597961425781,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            121
          ]
        }
      ],
      "orig": "where [sos] denotes the start-of-sequence token, r is the ground truth tokens, \u02c6 r is the predict tokens, and G represent",
      "text": "where [sos] denotes the start-of-sequence token, r is the ground truth tokens, \u02c6 r is the predict tokens, and G represent"
    },
    {
      "self_ref": "#/texts/97",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 317.3008117675781,
            "t": 532.4964599609375,
            "r": 554.6704711914062,
            "b": 475.7091064453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            288
          ]
        }
      ],
      "orig": "GPT-2 [29] model. However, such a 1D design is inadequate for our specific scenarios. Predicting long-term videos requires generating tens of thousands of tokens, which is significantly time-consuming. Additionally, it overlooks the spatially structured image features inherent in images.",
      "text": "GPT-2 [29] model. However, such a 1D design is inadequate for our specific scenarios. Predicting long-term videos requires generating tens of thousands of tokens, which is significantly time-consuming. Additionally, it overlooks the spatially structured image features inherent in images."
    },
    {
      "self_ref": "#/texts/98",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 316.6626281738281,
            "t": 471.7824401855469,
            "r": 554.5677490234375,
            "b": 403.04010009765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            349
          ]
        }
      ],
      "orig": "Therefore, we propose a next-state prediction pipeline, which consists of two modules: one integrates temporal and multimodal information for next-state feature generation (i.e. Temporal-multimodal Fusion Module), and the other is an autoregressive module (i.e. Internal-state Autoregressive Module) for high-quality internal-state token generation.",
      "text": "Therefore, we propose a next-state prediction pipeline, which consists of two modules: one integrates temporal and multimodal information for next-state feature generation (i.e. Temporal-multimodal Fusion Module), and the other is an autoregressive module (i.e. Internal-state Autoregressive Module) for high-quality internal-state token generation."
    },
    {
      "self_ref": "#/texts/99",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 317.2359924316406,
            "t": 399.1921691894531,
            "r": 554.6654663085938,
            "b": 282.55010986328125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            559
          ]
        }
      ],
      "orig": "Temporal-multimodal Fusion Module. Our temporalmultimodal module is composed of a separate temporal layer and a multimodal layer. This decouples the processing of temporal and multimodal information, thereby improving both training and inference speed while also reducing GPU memory consumption. As shown in Figure 3 (b), we propose to employ a causal attention mask in the temporal transformer layer Fa Fa (\u00b7), where each token only attends to itself and tokens at the same sequential position from all previous frames, fully leveraging temporal information.",
      "text": "Temporal-multimodal Fusion Module. Our temporalmultimodal module is composed of a separate temporal layer and a multimodal layer. This decouples the processing of temporal and multimodal information, thereby improving both training and inference speed while also reducing GPU memory consumption. As shown in Figure 3 (b), we propose to employ a causal attention mask in the temporal transformer layer Fa Fa (\u00b7), where each token only attends to itself and tokens at the same sequential position from all previous frames, fully leveraging temporal information."
    },
    {
      "self_ref": "#/texts/100",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 322.6793212890625,
            "t": 267.83203125,
            "r": 322.8427734375,
            "b": 260.9179992675781,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\\t",
      "text": "\\t"
    },
    {
      "self_ref": "#/texts/101",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 317.3795166015625,
            "t": 244.59495544433594,
            "r": 554.6815185546875,
            "b": 187.1510772705078,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            285
          ]
        }
      ],
      "orig": "In the multimodal information fusion layer FbFb(\u00b7), we employ a bidirectional mask in the same frame, which is designed to fully integrate internal-state multimodal information and facilitates interactions between modalities. Each token attends to other tokens from the same time step,",
      "text": "In the multimodal information fusion layer FbFb(\u00b7), we employ a bidirectional mask in the same frame, which is designed to fully integrate internal-state multimodal information and facilitates interactions between modalities. Each token attends to other tokens from the same time step,"
    },
    {
      "self_ref": "#/texts/102",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 387.8265686035156,
            "t": 175.2702178955078,
            "r": 387.9121398925781,
            "b": 173.52676391601562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\\m",
      "text": "\\m"
    },
    {
      "self_ref": "#/texts/103",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 317.11041259765625,
            "t": 148.51844787597656,
            "r": 553.4133911132812,
            "b": 127.59710693359375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            92
          ]
        }
      ],
      "orig": "The temporal and multimodal layers are alternately stacked for N layers to form this module.",
      "text": "The temporal and multimodal layers are alternately stacked for N layers to form this module."
    },
    {
      "self_ref": "#/texts/104",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 317.41259765625,
            "t": 123.75015258789062,
            "r": 554.6839599609375,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            217
          ]
        }
      ],
      "orig": "Internal-state Autoregressive Module. After the temporalmultimodal module, we obtain features for future frame state prediction. A naive approach is to predict nextstate tokens h t at the same time. Recently, multiple",
      "text": "Internal-state Autoregressive Module. After the temporalmultimodal module, we obtain features for future frame state prediction. A naive approach is to predict nextstate tokens h t at the same time. Recently, multiple"
    },
    {
      "self_ref": "#/texts/105",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 303.6285400390625,
            "t": 57.846717834472656,
            "r": 308.21136474609375,
            "b": 51.11199951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "4",
      "text": "4"
    },
    {
      "self_ref": "#/texts/106",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 58.44431686401367,
            "t": 493.6436767578125,
            "r": 553.477783203125,
            "b": 419.7222900390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            983
          ]
        }
      ],
      "orig": "Figure 3. Inference illustration of vanilla GPT and temporal-aware GPT (ours). For simplicity, we assume a video clip only has 3 frames and each frame consists of only 3 tokens, where x 1 2 denotes 1 st token of the 2 nd video frame. (a) The vanilla GPT places all tokens in a 1D sequence and employ the causal attention, which can autoregressively predicts next tokens. (b) We propose a temporal-multimodal fusion module to meld multi-modal information {h i t } n i=1 and obtain the next-state feature { \u02da h i t+1 } n i=1 . To generate high-quality next-state videos and vehicle tokens, we employ the causal attention, thus such tokens ({ \u02c6 r i t+1 } n i=1 ) are autoregressively predicted. Emb(\u00b7) denotes the embedding of corresponding tokens. In the temporal layer, each token only attends to itself and tokens at the same sequential position from all previous frames. The multi-modal layer and internal-state autoregressive module are separately operated to the tokens per frame.",
      "text": "Figure 3. Inference illustration of vanilla GPT and temporal-aware GPT (ours). For simplicity, we assume a video clip only has 3 frames and each frame consists of only 3 tokens, where x 1 2 denotes 1 st token of the 2 nd video frame. (a) The vanilla GPT places all tokens in a 1D sequence and employ the causal attention, which can autoregressively predicts next tokens. (b) We propose a temporal-multimodal fusion module to meld multi-modal information {h i t } n i=1 and obtain the next-state feature { \u02da h i t+1 } n i=1 . To generate high-quality next-state videos and vehicle tokens, we employ the causal attention, thus such tokens ({ \u02c6 r i t+1 } n i=1 ) are autoregressively predicted. Emb(\u00b7) denotes the embedding of corresponding tokens. In the temporal layer, each token only attends to itself and tokens at the same sequential position from all previous frames. The multi-modal layer and internal-state autoregressive module are separately operated to the tokens per frame."
    },
    {
      "self_ref": "#/texts/107",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 233.08370971679688,
            "t": 715.4698486328125,
            "r": 252.08372497558594,
            "b": 671.0919189453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            16
          ]
        }
      ],
      "orig": "Multi-modalLayer",
      "text": "Multi-modalLayer"
    },
    {
      "self_ref": "#/texts/108",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 366.1191711425781,
            "t": 693.8733520507812,
            "r": 375.3660583496094,
            "b": 692.59375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/109",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 378.18463134765625,
            "t": 693.8733520507812,
            "r": 387.4315185546875,
            "b": 692.59375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "\u00b7\u00b7\u00b7",
      "text": "\u00b7\u00b7\u00b7"
    },
    {
      "self_ref": "#/texts/110",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 508.2198791503906,
            "t": 715.2852783203125,
            "r": 530.7158203125,
            "b": 671.7257690429688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            34
          ]
        }
      ],
      "orig": "Internal-stateAutoregressiveModule",
      "text": "Internal-stateAutoregressiveModule"
    },
    {
      "self_ref": "#/texts/111",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 365.7232666015625,
            "t": 621.6221313476562,
            "r": 373.8587646484375,
            "b": 554.1832885742188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            17
          ]
        }
      ],
      "orig": "Multi-modal Layer",
      "text": "Multi-modal Layer"
    },
    {
      "self_ref": "#/texts/112",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 279.3584899902344,
            "t": 715.469970703125,
            "r": 344.6332702636719,
            "b": 671.092041015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            32
          ]
        }
      ],
      "orig": "Te TemporalLayerMulti-modalLayer",
      "text": "Te TemporalLayerMulti-modalLayer"
    },
    {
      "self_ref": "#/texts/113",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 187.49456787109375,
            "t": 682.41015625,
            "r": 193.44349670410156,
            "b": 677.4880981445312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "Te",
      "text": "Te"
    },
    {
      "self_ref": "#/texts/114",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 187.21128845214844,
            "t": 710.809326171875,
            "r": 206.21131896972656,
            "b": 682.3394165039062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "TemporalLayer",
      "text": "TemporalLayer"
    },
    {
      "self_ref": "#/texts/115",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 410.82073974609375,
            "t": 681.6052856445312,
            "r": 416.7696533203125,
            "b": 676.6832275390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "Te",
      "text": "Te"
    },
    {
      "self_ref": "#/texts/116",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 410.5374450683594,
            "t": 715.6187744140625,
            "r": 476.8704528808594,
            "b": 671.2408447265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            29
          ]
        }
      ],
      "orig": "TemporalLayerMulti-modalLayer",
      "text": "TemporalLayerMulti-modalLayer"
    },
    {
      "self_ref": "#/texts/117",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 107.79165649414062,
            "t": 656.5758666992188,
            "r": 115.92716979980469,
            "b": 573.6007080078125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "Autoregressive Module",
      "text": "Autoregressive Module"
    },
    {
      "self_ref": "#/texts/118",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 191.95005798339844,
            "t": 591.1651000976562,
            "r": 211.38063049316406,
            "b": 585.26123046875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "reshape",
      "text": "reshape"
    },
    {
      "self_ref": "#/texts/119",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 276.4518737792969,
            "t": 591.1651000976562,
            "r": 295.8824462890625,
            "b": 585.26123046875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "reshape",
      "text": "reshape"
    },
    {
      "self_ref": "#/texts/120",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 240.8634490966797,
            "t": 558.5477905273438,
            "r": 246.8123779296875,
            "b": 553.625732421875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "Te",
      "text": "Te"
    },
    {
      "self_ref": "#/texts/121",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 240.58016967773438,
            "t": 586.9469604492188,
            "r": 248.69796752929688,
            "b": 558.47705078125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "Temporal",
      "text": "Temporal"
    },
    {
      "self_ref": "#/texts/122",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 240.58016967773438,
            "t": 622.2438354492188,
            "r": 246.92745971679688,
            "b": 588.9492797851562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            9
          ]
        }
      ],
      "orig": "Attention",
      "text": "Attention"
    },
    {
      "self_ref": "#/texts/123",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 319.43634033203125,
            "t": 589.6392211914062,
            "r": 331.68829345703125,
            "b": 585.2483520507812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            4
          ]
        }
      ],
      "orig": "copy",
      "text": "copy"
    },
    {
      "self_ref": "#/texts/124",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 406.3525085449219,
            "t": 588.8343505859375,
            "r": 418.6044616699219,
            "b": 584.4434814453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            4
          ]
        }
      ],
      "orig": "copy",
      "text": "copy"
    },
    {
      "self_ref": "#/texts/125",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 514.2049560546875,
            "t": 568.35107421875,
            "r": 520.5523071289062,
            "b": 522.1760864257812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "Internal-state",
      "text": "Internal-state"
    },
    {
      "self_ref": "#/texts/126",
      "parent": {
        "$ref": "#/pictures/2"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 514.2049560546875,
            "t": 653.4218139648438,
            "r": 522.3405151367188,
            "b": 570.4466552734375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "Autoregressive Module",
      "text": "Autoregressive Module"
    },
    {
      "self_ref": "#/texts/127",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 59.02541732788086,
            "t": 504.97967529296875,
            "r": 176.40435791015625,
            "b": 497.599853515625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            40
          ]
        }
      ],
      "orig": "(a)  VaVanilla GPT-T-based WoWorld Model",
      "text": "(a)  VaVanilla GPT-T-based WoWorld Model"
    },
    {
      "self_ref": "#/texts/128",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "paragraph",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 298.99407958984375,
            "t": 504.97967529296875,
            "r": 432.0060119628906,
            "b": 497.599853515625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            41
          ]
        }
      ],
      "orig": "(b) TeTemporal-aware WoWorld Model (Ours)",
      "text": "(b) TeTemporal-aware WoWorld Model (Ours)"
    },
    {
      "self_ref": "#/texts/129",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 58.3203125,
            "t": 406.63543701171875,
            "r": 295.9349365234375,
            "b": 206.07247924804688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1028
          ]
        }
      ],
      "orig": "image-generation works [33] propose that an autoregressive pipeline for next-token prediction generates better images, and even outperforms diffusion methods. Inspired by this, we propose an internal-state autoregressive module to generate the pose and image for the next time step (see Figure 3 (b)). Specifically, to predict \u02c6 rT +1 = ( \u02c6 r 1 T +1, . . . , \u02c6 r H\u00d7W+2 T +1 ), we add the temporal-multimodal output feature \u02da h T = ( \u02da h 1 T, . . . , \u02da h H\u00d7W+2 T ) with the sequential tokens ([sos] , \u02c6 r 1 T +1, . . . , \u02c6 r H\u00d7W+1 T +1 ). Then they are input to the internal-state autoregressive transformer layers Fc Fc (\u00b7) . The causal mask is employed in these layers, thus each token can only attend itself and prefix internal-state tokens. The autoregressive process is present in Eq. 6. As our pipeline incorporates both the next-state prediction and the next internalstate token prediction, we enforce two teacher-forcing strategies in training, i.e. one for the frame level and the other one for the internal-state level.",
      "text": "image-generation works [33] propose that an autoregressive pipeline for next-token prediction generates better images, and even outperforms diffusion methods. Inspired by this, we propose an internal-state autoregressive module to generate the pose and image for the next time step (see Figure 3 (b)). Specifically, to predict \u02c6 rT +1 = ( \u02c6 r 1 T +1, . . . , \u02c6 r H\u00d7W+2 T +1 ), we add the temporal-multimodal output feature \u02da h T = ( \u02da h 1 T, . . . , \u02da h H\u00d7W+2 T ) with the sequential tokens ([sos] , \u02c6 r 1 T +1, . . . , \u02c6 r H\u00d7W+1 T +1 ). Then they are input to the internal-state autoregressive transformer layers Fc Fc (\u00b7) . The causal mask is employed in these layers, thus each token can only attend itself and prefix internal-state tokens. The autoregressive process is present in Eq. 6. As our pipeline incorporates both the next-state prediction and the next internalstate token prediction, we enforce two teacher-forcing strategies in training, i.e. one for the frame level and the other one for the internal-state level."
    },
    {
      "self_ref": "#/texts/130",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 284.2792053222656,
            "t": 171.6104278564453,
            "r": 294.9391784667969,
            "b": 163.04258728027344,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "(6)",
      "text": "(6)"
    },
    {
      "self_ref": "#/texts/131",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 58.081809997558594,
            "t": 150.30245971679688,
            "r": 221.90675354003906,
            "b": 141.33612060546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            42
          ]
        }
      ],
      "orig": "We use cross-entropy loss for training, as",
      "text": "We use cross-entropy loss for training, as"
    },
    {
      "self_ref": "#/texts/132",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 65.66083526611328,
            "t": 119.81063842773438,
            "r": 65.80374908447266,
            "b": 112.57778930664062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\\m",
      "text": "\\m"
    },
    {
      "self_ref": "#/texts/133",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 58.34602737426758,
            "t": 88.33704376220703,
            "r": 295.81671142578125,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            66
          ]
        }
      ],
      "orig": "where r is the ground truth tokens, and \u02c6 r is the predict tokens.",
      "text": "where r is the ground truth tokens, and \u02c6 r is the predict tokens."
    },
    {
      "self_ref": "#/texts/134",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 317.42535400390625,
            "t": 407.3707275390625,
            "r": 375.26641845703125,
            "b": 399.68853759765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            12
          ]
        }
      ],
      "orig": "3.3. Decoder",
      "text": "3.3. Decoder",
      "level": 1
    },
    {
      "self_ref": "#/texts/135",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 316.8912353515625,
            "t": 392.43701171875,
            "r": 554.6727905273438,
            "b": 308.0050964355469,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            430
          ]
        }
      ],
      "orig": "The next-state tokens \u02c6 rT +1 = (\u03d5 \u02c6 T +1, v \u02c6 T +1, q \u02c6 T +1 ) are predicted using the world model, and then we can leverage the decoder to generate the corresponding relative orientation \u2206 \u02c6 \u03b8 T +1 , relative location (\u2206\u02c6xxT +1 , \u2206\u02c6yyT +1), and the reconstructed image \u02c6 I T +1 for that state. This process allows us to map the predicted latent representations back into physical outputs, including both spatial and visual data.",
      "text": "The next-state tokens \u02c6 rT +1 = (\u03d5 \u02c6 T +1, v \u02c6 T +1, q \u02c6 T +1 ) are predicted using the world model, and then we can leverage the decoder to generate the corresponding relative orientation \u2206 \u02c6 \u03b8 T +1 , relative location (\u2206\u02c6xxT +1 , \u2206\u02c6yyT +1), and the reconstructed image \u02c6 I T +1 for that state. This process allows us to map the predicted latent representations back into physical outputs, including both spatial and visual data."
    },
    {
      "self_ref": "#/texts/136",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 317.0478515625,
            "t": 305.0961608886719,
            "r": 553.3521118164062,
            "b": 260.194091796875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            189
          ]
        }
      ],
      "orig": "Vehicle Pose Decoder. For the predicted relative orientation token protect \\hat  {\\phi }_{T+1} nd relative loc the corresponding values through the inverse function of the Eq. 2 as follows:",
      "text": "Vehicle Pose Decoder. For the predicted relative orientation token protect \\hat  {\\phi }_{T+1} nd relative loc the corresponding values through the inverse function of the Eq. 2 as follows:"
    },
    {
      "self_ref": "#/texts/137",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 543.0292358398438,
            "t": 217.8327178955078,
            "r": 553.689208984375,
            "b": 209.34458923339844,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "(8)",
      "text": "(8)"
    },
    {
      "self_ref": "#/texts/138",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 317.1103820800781,
            "t": 166.9984588623047,
            "r": 554.6765747070312,
            "b": 110.21112060546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            283
          ]
        }
      ],
      "orig": "Temporal-aware Decoder. For the predicted image tokens q \u02c6 T +1 , we retrieve the corresponding feature from the codebook protect \\mathcal  {Z} \\in \\mathbb {R}^ Tokenizer. Note that after the quantization layer we insert a temporal self-attention to enhance the temporal consistency.",
      "text": "Temporal-aware Decoder. For the predicted image tokens q \u02c6 T +1 , we retrieve the corresponding feature from the codebook protect \\mathcal  {Z} \\in \\mathbb {R}^ Tokenizer. Note that after the quantization layer we insert a temporal self-attention to enhance the temporal consistency."
    },
    {
      "self_ref": "#/texts/139",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 329.0666809082031,
            "t": 90.54304504394531,
            "r": 329.1455383300781,
            "b": 89.00880432128906,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "\\h",
      "text": "\\h"
    },
    {
      "self_ref": "#/texts/140",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 303.8278503417969,
            "t": 57.96626663208008,
            "r": 307.8726501464844,
            "b": 50.982486724853516,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "5",
      "text": "5"
    },
    {
      "self_ref": "#/texts/141",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.67534255981445,
            "t": 717.6095581054688,
            "r": 245.31639099121094,
            "b": 707.7904052734375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            38
          ]
        }
      ],
      "orig": "3.4. Long-term Controllable Generation",
      "text": "3.4. Long-term Controllable Generation",
      "level": 1
    },
    {
      "self_ref": "#/texts/142",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.348331451416016,
            "t": 699.6861572265625,
            "r": 295.9309997558594,
            "b": 523.2681274414062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            879
          ]
        }
      ],
      "orig": "Token Dropout for Drifting-free Autoregression. During training, the world model uses past ground-truth tokens as conditioning to predict the next tokens. However, during inference, the model must rely on previously generated tokens for conditioning, which may contain imperfections. Training solely with perfect ground-truth images can lead to a content drifting problem during inference, causing rapid degradation and eventual failure in the generated outputs. To address this, we propose a random masking strategy (RMS), where some tokens from ground-truth tokens are randomly dropped out. Each token has a 50% chance of being replaced by another random token in this frame, and this dropout is applied to the entire conditioning image sequence with a probability of 30%. As shown in Figure 4, this dropout strategy significantly mitigates the drifting issue during inference.",
      "text": "Token Dropout for Drifting-free Autoregression. During training, the world model uses past ground-truth tokens as conditioning to predict the next tokens. However, during inference, the model must rely on previously generated tokens for conditioning, which may contain imperfections. Training solely with perfect ground-truth images can lead to a content drifting problem during inference, causing rapid degradation and eventual failure in the generated outputs. To address this, we propose a random masking strategy (RMS), where some tokens from ground-truth tokens are randomly dropped out. Each token has a 50% chance of being replaced by another random token in this frame, and this dropout is applied to the entire conditioning image sequence with a probability of 30%. As shown in Figure 4, this dropout strategy significantly mitigates the drifting issue during inference."
    },
    {
      "self_ref": "#/texts/143",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.530128479003906,
            "t": 520.0611572265625,
            "r": 295.9302978515625,
            "b": 331.69708251953125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            926
          ]
        }
      ],
      "orig": "Balanced Attention for Precise Control. The world model utilizes extensive attention operations to exchange and fuse information among tokens. However, each front view image is discretized into 512 tokens, while only 2 tokens represent the pose (orientation and location). This imbalance can cause the model to overlook pose signals, leading to unsatisfactory controllable generation. To address this, we propose a balanced attention operation to achieve more precise control by prioritizing ego state tokens in the attention mechanism, rather than attending to all tokens equally. Specifically, we manually increase the weights of the orientation and location tokens in the attention map (before the softmax layer), adding constant weights of 0 . 4 and 0 . 2, respectively, to these tokens. Additionally, we incorporate QK-norm [16] and 2Drotary positional encoding [32] to further stabilize training and enhance performance.",
      "text": "Balanced Attention for Precise Control. The world model utilizes extensive attention operations to exchange and fuse information among tokens. However, each front view image is discretized into 512 tokens, while only 2 tokens represent the pose (orientation and location). This imbalance can cause the model to overlook pose signals, leading to unsatisfactory controllable generation. To address this, we propose a balanced attention operation to achieve more precise control by prioritizing ego state tokens in the attention mechanism, rather than attending to all tokens equally. Specifically, we manually increase the weights of the orientation and location tokens in the attention map (before the softmax layer), adding constant weights of 0 . 4 and 0 . 2, respectively, to these tokens. Additionally, we incorporate QK-norm [16] and 2Drotary positional encoding [32] to further stabilize training and enhance performance."
    },
    {
      "self_ref": "#/texts/144",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.727149963378906,
            "t": 317.3040466308594,
            "r": 135.20455932617188,
            "b": 306.6041259765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "4. Experiments",
      "text": "4. Experiments",
      "level": 1
    },
    {
      "self_ref": "#/texts/145",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.70821762084961,
            "t": 297.38861083984375,
            "r": 187.25611877441406,
            "b": 287.58038330078125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "4.1. Implementation Details",
      "text": "4.1. Implementation Details",
      "level": 1
    },
    {
      "self_ref": "#/texts/146",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.36375427246094,
            "t": 279.46514892578125,
            "r": 295.9227294921875,
            "b": 174.7781219482422,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            500
          ]
        }
      ],
      "orig": "Tokenizer and Decoder. The video tokenizer has 70M parameters. The size of adopted codebook is set to 16 , 384 . The model is trained for 1 , 000K steps with a total batch size of 128 on 32 NVIDIA 4090 GPUs, using images from Openimages [22], COCO [25], YoutubeDV [45], and NuPlan [5] datasets. We train the temporal-aware VQVAE using a combination of three loss functions: charbonnier loss [23], perceptual loss from LPIPS [46], and codebook loss [36] (see supplementary materials for more details).",
      "text": "Tokenizer and Decoder. The video tokenizer has 70M parameters. The size of adopted codebook is set to 16 , 384 . The model is trained for 1 , 000K steps with a total batch size of 128 on 32 NVIDIA 4090 GPUs, using images from Openimages [22], COCO [25], YoutubeDV [45], and NuPlan [5] datasets. We train the temporal-aware VQVAE using a combination of three loss functions: charbonnier loss [23], perceptual loss from LPIPS [46], and codebook loss [36] (see supplementary materials for more details)."
    },
    {
      "self_ref": "#/texts/147",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 58.193748474121094,
            "t": 171.49046325683594,
            "r": 295.9301452636719,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            471
          ]
        }
      ],
      "orig": "World Model. The world model has 1B parameters and is trained on video sequences. The model is conditioned on 15 frames to predict the next frame. It is trained on over 3456 hours of human driving data, as shown in Table 1. 120 hours come from the publicly available NuPlan [5] dataset, and 3336 hours consist of private data (see supplementary materials for more details). Training is conducted over 12 days, spanning 450K iterations with a batch size of 64, distributed",
      "text": "World Model. The world model has 1B parameters and is trained on video sequences. The model is conditioned on 15 frames to predict the next frame. It is trained on over 3456 hours of human driving data, as shown in Table 1. 120 hours come from the publicly available NuPlan [5] dataset, and 3336 hours consist of private data (see supplementary materials for more details). Training is conducted over 12 days, spanning 450K iterations with a batch size of 64, distributed"
    },
    {
      "self_ref": "#/texts/148",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 303.84771728515625,
            "t": 57.92641830444336,
            "r": 308.1715087890625,
            "b": 50.982486724853516,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "6",
      "text": "6"
    },
    {
      "self_ref": "#/texts/149",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.1213684082031,
            "t": 718.5247802734375,
            "r": 553.32177734375,
            "b": 698.3423461914062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            106
          ]
        }
      ],
      "orig": "Table 1. Real-world driving world models. Trained on large-scale high-quality driving data. Private data .",
      "text": "Table 1. Real-world driving world models. Trained on large-scale high-quality driving data. Private data ."
    },
    {
      "self_ref": "#/texts/150",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.6186218261719,
            "t": 542.5776977539062,
            "r": 445.21917724609375,
            "b": 535.623779296875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "across 64 NVIDIA A100 GPUs.",
      "text": "across 64 NVIDIA A100 GPUs."
    },
    {
      "self_ref": "#/texts/151",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.4093933105469,
            "t": 530.7621459960938,
            "r": 554.6675415039062,
            "b": 449.985107421875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            401
          ]
        }
      ],
      "orig": "Evaluation Dataset and Metrics. We use 200 video clips from the NuPlan [5] test dataset as our test set. Additionally, we include 150 video clips from the NuScenes [4] test set as part of our evaluation following Vista [9]. The quality of video generation is assessed using the Frechet Video Distance (FVD), and we also report the Frechet Inception Distance (FID) to evaluate image generation quality.",
      "text": "Evaluation Dataset and Metrics. We use 200 video clips from the NuPlan [5] test dataset as our test set. Additionally, we include 150 video clips from the NuScenes [4] test set as part of our evaluation following Vista [9]. The quality of video generation is assessed using the Frechet Video Distance (FVD), and we also report the Frechet Inception Distance (FID) to evaluate image generation quality."
    },
    {
      "self_ref": "#/texts/152",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.4582214355469,
            "t": 440.25360107421875,
            "r": 468.27459716796875,
            "b": 430.44537353515625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            30
          ]
        }
      ],
      "orig": "4.2. Comparison and Evaluation",
      "text": "4.2. Comparison and Evaluation",
      "level": 1
    },
    {
      "self_ref": "#/texts/153",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.11376953125,
            "t": 422.629150390625,
            "r": 554.684814453125,
            "b": 246.21011352539062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            878
          ]
        }
      ],
      "orig": "Long-time Video Generation. One of the key advantages of our method is its ability to generate long-duration videos. As shown in Figure 5, we visualize one long-duration video generated by our model. By conditioning on just 15 frames, our model can generate up to 640 future frames at 10 Hz, resulting in 64-second videos with strong temporal consistency. These results demonstrate that our model maintains high video fidelity and preserves 3D structural integrity across the generated frames. In contrast, previous methods often struggle with drifting or degradation in long-duration videos. The ability to generate extended video sequences underscores our model's potential for tasks that require long-term predictions, such as autonomous driving or video synthesis in complex dynamic environments. More long-time generation videos are provided in the supplementary materials.",
      "text": "Long-time Video Generation. One of the key advantages of our method is its ability to generate long-duration videos. As shown in Figure 5, we visualize one long-duration video generated by our model. By conditioning on just 15 frames, our model can generate up to 640 future frames at 10 Hz, resulting in 64-second videos with strong temporal consistency. These results demonstrate that our model maintains high video fidelity and preserves 3D structural integrity across the generated frames. In contrast, previous methods often struggle with drifting or degradation in long-duration videos. The ability to generate extended video sequences underscores our model's potential for tasks that require long-term predictions, such as autonomous driving or video synthesis in complex dynamic environments. More long-time generation videos are provided in the supplementary materials."
    },
    {
      "self_ref": "#/texts/154",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.1044006347656,
            "t": 243.30116271972656,
            "r": 554.6849365234375,
            "b": 102.74812316894531,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            682
          ]
        }
      ],
      "orig": "Quantitative Comparison of Generated Videos. We provide the quantitative comparison with several methods on the NuScenes [4] dataset in Table 2. Since most methods are not publicly available, we use the results reported in their respective papers for comparison. Although NuScenes is zero-shot to our model, we still achieve the comparable performance with the state-of-the-art method (Vista [9]). Most of the previous methods are based on a well-trained Stable Video Diffusion (SVD) [2], which is trained on over billions of images, while our designed GPT-style framework is trained from scratch. Moreover, our method is capable of generating significantly longer videos than them.",
      "text": "Quantitative Comparison of Generated Videos. We provide the quantitative comparison with several methods on the NuScenes [4] dataset in Table 2. Since most methods are not publicly available, we use the results reported in their respective papers for comparison. Although NuScenes is zero-shot to our model, we still achieve the comparable performance with the state-of-the-art method (Vista [9]). Most of the previous methods are based on a well-trained Stable Video Diffusion (SVD) [2], which is trained on over billions of images, while our designed GPT-style framework is trained from scratch. Moreover, our method is capable of generating significantly longer videos than them."
    },
    {
      "self_ref": "#/texts/155",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 317.5917053222656,
            "t": 99.83915710449219,
            "r": 553.3051147460938,
            "b": 78.84807586669922,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            112
          ]
        }
      ],
      "orig": "Qualitative Comparison of Generated Videos. We provide a qualitative comparison with SVD [2] on the NuScenes [4]",
      "text": "Qualitative Comparison of Generated Videos. We provide a qualitative comparison with SVD [2] on the NuScenes [4]"
    },
    {
      "self_ref": "#/texts/156",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.365291595458984,
            "t": 591.8598022460938,
            "r": 553.3833618164062,
            "b": 539.5632934570312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            625
          ]
        }
      ],
      "orig": "Figure 4. The effect of our proposed masking strategy. Our masking strategy effectively mitigates autoregressive drifting; without it, the world model experiences severe content drift, causing generated videos to degrade rapidly after the 10th frame. Table 2. Comparisons on the NuScenes [4] validation set. We compare with existing methods on NuScenes. Blue denotes NuScenes is enclosed in their training set, while orange represents zero-shot testing. Our \"DrivingWorld\" achieve comparable performance with state-of-the-art methods and generate much longer videos.\"DrivingWorld (w/o P)\" means training without private data.",
      "text": "Figure 4. The effect of our proposed masking strategy. Our masking strategy effectively mitigates autoregressive drifting; without it, the world model experiences severe content drift, causing generated videos to degrade rapidly after the 10th frame. Table 2. Comparisons on the NuScenes [4] validation set. We compare with existing methods on NuScenes. Blue denotes NuScenes is enclosed in their training set, while orange represents zero-shot testing. Our \"DrivingWorld\" achieve comparable performance with state-of-the-art methods and generate much longer videos.\"DrivingWorld (w/o P)\" means training without private data."
    },
    {
      "self_ref": "#/texts/157",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.7175178527832,
            "t": 710.6878662109375,
            "r": 66.3814697265625,
            "b": 672.4524536132812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            12
          ]
        }
      ],
      "orig": "(a) Drifting",
      "text": "(a) Drifting"
    },
    {
      "self_ref": "#/texts/158",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.578060150146484,
            "t": 653.5357055664062,
            "r": 66.24201202392578,
            "b": 597.7128295898438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            17
          ]
        }
      ],
      "orig": "(b) Drifting Free",
      "text": "(b) Drifting Free"
    },
    {
      "self_ref": "#/texts/159",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 73.27641296386719,
            "t": 717.9970092773438,
            "r": 75.4769515991211,
            "b": 712.2975463867188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/160",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 73.25105285644531,
            "t": 654.0340576171875,
            "r": 75.45159149169922,
            "b": 648.3345947265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/161",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 193.7346649169922,
            "t": 717.891357421875,
            "r": 197.520263671875,
            "b": 712.19189453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "4",
      "text": "4"
    },
    {
      "self_ref": "#/texts/162",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 193.7346649169922,
            "t": 653.8016357421875,
            "r": 197.520263671875,
            "b": 648.1021728515625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "4",
      "text": "4"
    },
    {
      "self_ref": "#/texts/163",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 315.11798095703125,
            "t": 717.8789672851562,
            "r": 318.6506652832031,
            "b": 712.1879272460938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "7",
      "text": "7"
    },
    {
      "self_ref": "#/texts/164",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 315.11798095703125,
            "t": 653.68359375,
            "r": 318.6506652832031,
            "b": 647.9925537109375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "7",
      "text": "7"
    },
    {
      "self_ref": "#/texts/165",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 438.4145202636719,
            "t": 718.0181274414062,
            "r": 445.5874328613281,
            "b": 712.2258911132812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "10",
      "text": "10"
    },
    {
      "self_ref": "#/texts/166",
      "parent": {
        "$ref": "#/pictures/3"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 438.3405456542969,
            "t": 653.9812622070312,
            "r": 445.5134582519531,
            "b": 648.1890258789062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "10",
      "text": "10"
    },
    {
      "self_ref": "#/texts/167",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.37747573852539,
            "t": 469.8687744140625,
            "r": 295.8160095214844,
            "b": 441.66534423828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            128
          ]
        }
      ],
      "orig": "Table 3. Quantitative comparison of different VQVAE methods. The evaluations are performed on the 256 \u00d7 512 NuPlan [5] datasets.",
      "text": "Table 3. Quantitative comparison of different VQVAE methods. The evaluations are performed on the 256 \u00d7 512 NuPlan [5] datasets."
    },
    {
      "self_ref": "#/texts/168",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.629512786865234,
            "t": 340.25543212890625,
            "r": 295.9267578125,
            "b": 307.3691101074219,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            162
          ]
        }
      ],
      "orig": "dataset. As shown in Figure 6, our generated videos demonstrate superior temporal consistency, particularly in maintaining details like street lanes and vehicles.",
      "text": "dataset. As shown in Figure 6, our generated videos demonstrate superior temporal consistency, particularly in maintaining details like street lanes and vehicles."
    },
    {
      "self_ref": "#/texts/169",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.37747573852539,
            "t": 291.9597473144531,
            "r": 295.8186340332031,
            "b": 251.4892120361328,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            203
          ]
        }
      ],
      "orig": "Table 4. Comparison of w/ and w/o Random Masking Strategy. Removing the random masking strategy during training (\"w/o Masking\") leads to drifting, resulting in degraded performance on NuPlan [5] dataset.",
      "text": "Table 4. Comparison of w/ and w/o Random Masking Strategy. Removing the random masking strategy during training (\"w/o Masking\") leads to drifting, resulting in degraded performance on NuPlan [5] dataset."
    },
    {
      "self_ref": "#/texts/170",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.52928924560547,
            "t": 183.52516174316406,
            "r": 295.92431640625,
            "b": 79.60523986816406,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            495
          ]
        }
      ],
      "orig": "Quantitative Comparison of Image Tokenizers. We further evaluate our temporal-aware image tokenizer against those proposed in other works. Since the image tokenizer is part of a VQVAE, we assess the encoding-decoding performance of these VQVAEs instead. The experiments, conducted on the NuPlan [5] dataset, are summarized in Table 3. The VQVAE models from VAR [34] and VQGAN [8] demonstrate reasonable image quality in terms of PSNR and LPIPS scores, but both fall short on FID and FVD metrics.",
      "text": "Quantitative Comparison of Image Tokenizers. We further evaluate our temporal-aware image tokenizer against those proposed in other works. Since the image tokenizer is part of a VQVAE, we assess the encoding-decoding performance of these VQVAEs instead. The experiments, conducted on the NuPlan [5] dataset, are summarized in Table 3. The VQVAE models from VAR [34] and VQGAN [8] demonstrate reasonable image quality in terms of PSNR and LPIPS scores, but both fall short on FID and FVD metrics."
    },
    {
      "self_ref": "#/texts/171",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 317.3678894042969,
            "t": 468.1864318847656,
            "r": 553.5081787109375,
            "b": 399.444091796875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            328
          ]
        }
      ],
      "orig": "In contrast, Llama-Gen's VQVAE [33] shows significant improvements in FID and FVD scores. After fine-tuning it on driving scenes, we observe further enhancements in FVD performance. Ultimately, our temporal-aware VQVAE outperforms all others, enhancing temporal consistency and achieving the best scores across all four metrics.",
      "text": "In contrast, Llama-Gen's VQVAE [33] shows significant improvements in FID and FVD scores. After fine-tuning it on driving scenes, we observe further enhancements in FVD performance. Ultimately, our temporal-aware VQVAE outperforms all others, enhancing temporal consistency and achieving the best scores across all four metrics."
    },
    {
      "self_ref": "#/texts/172",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 317.4582214355469,
            "t": 384.8765563964844,
            "r": 407.15679931640625,
            "b": 375.057373046875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            19
          ]
        }
      ],
      "orig": "4.3. Ablation Study",
      "text": "4.3. Ablation Study",
      "level": 1
    },
    {
      "self_ref": "#/texts/173",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 317.3702697753906,
            "t": 365.7881164550781,
            "r": 554.66064453125,
            "b": 273.0461120605469,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            453
          ]
        }
      ],
      "orig": "Setting. Due to the prolonged training time and computational costs, we experiment on a smaller dataset for the ablation study. We extract 12 hours of video data from the NuPlan [5] dataset for training, and select 20 videos from NuPlan [5] test sets to create our testing data. All ablations are conducted on 32 NVIDIA A100 GPUs with a total batch size of 32. Each model is trained from scratch for 50K iterations, requiring approximately 32 GPU hours.",
      "text": "Setting. Due to the prolonged training time and computational costs, we experiment on a smaller dataset for the ablation study. We extract 12 hours of video data from the NuPlan [5] dataset for training, and select 20 videos from NuPlan [5] test sets to create our testing data. All ablations are conducted on 32 NVIDIA A100 GPUs with a total batch size of 32. Each model is trained from scratch for 50K iterations, requiring approximately 32 GPU hours."
    },
    {
      "self_ref": "#/texts/174",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 317.09747314453125,
            "t": 268.6841125488281,
            "r": 554.8475341796875,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            930
          ]
        }
      ],
      "orig": "Model Structure w/ and w/o Random Masking Strategy. To evaluate the impact of our random masking strategy on model robustness, we experiment model training with and without random token masking. This masking process simulates potential prediction errors during inference, enhancing the model's ability to handle noise. As shown in Table 4 , the model trained without masking experiences a significant performance decline on NuPlan [5] dataset, particularly in long term videos where inference errors are more prevalent as we can see from the FVD 40 scores. Therefore, disabling masking results in a substantial increase in FVD, with a rise of 4 to 32 percent across different scenarios, indicating poor generalization and reduced robustness against noisy inputs. Discussion With Vanilla GPT structure. We compare the memory usage of our DrivingWorld structure with the vanilla GPT architecture, specifically GPT-2 [29], which pro-",
      "text": "Model Structure w/ and w/o Random Masking Strategy. To evaluate the impact of our random masking strategy on model robustness, we experiment model training with and without random token masking. This masking process simulates potential prediction errors during inference, enhancing the model's ability to handle noise. As shown in Table 4 , the model trained without masking experiences a significant performance decline on NuPlan [5] dataset, particularly in long term videos where inference errors are more prevalent as we can see from the FVD 40 scores. Therefore, disabling masking results in a substantial increase in FVD, with a rise of 4 to 32 percent across different scenarios, indicating poor generalization and reduced robustness against noisy inputs. Discussion With Vanilla GPT structure. We compare the memory usage of our DrivingWorld structure with the vanilla GPT architecture, specifically GPT-2 [29], which pro-"
    },
    {
      "self_ref": "#/texts/175",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 303.708251953125,
            "t": 57.70724105834961,
            "r": 307.9822082519531,
            "b": 51.04226303100586,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "7",
      "text": "7"
    },
    {
      "self_ref": "#/texts/176",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.87627410888672,
            "t": 637.3773803710938,
            "r": 65.66650390625,
            "b": 574.85546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "rm Generation",
      "text": "rm Generation"
    },
    {
      "self_ref": "#/texts/177",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 59.18317794799805,
            "t": 571.3793334960938,
            "r": 67.59423828125,
            "b": 551.4786376953125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            4
          ]
        }
      ],
      "orig": "Long",
      "text": "Long"
    },
    {
      "self_ref": "#/texts/178",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.60974884033203,
            "t": 465.123779296875,
            "r": 552.9994506835938,
            "b": 446.0232849121094,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            264
          ]
        }
      ],
      "orig": "Figure 5. Long duration video generation. We present some videos generated by our method, each with 640 frames at 5Hz, i.e. 128 seconds. Please notice the coherent 3D scene structures captured by our method across different frames (please see the digital version).",
      "text": "Figure 5. Long duration video generation. We present some videos generated by our method, each with 640 frames at 5Hz, i.e. 128 seconds. Please notice the coherent 3D scene structures captured by our method across different frames (please see the digital version)."
    },
    {
      "self_ref": "#/texts/179",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 63.029056549072266,
            "t": 574.4014892578125,
            "r": 63.72917938232422,
            "b": 571.9846801757812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "-",
      "text": "-"
    },
    {
      "self_ref": "#/texts/180",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 75.00493621826172,
            "t": 717.3296508789062,
            "r": 77.50435638427734,
            "b": 710.8560791015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/181",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 197.613037109375,
            "t": 717.2456665039062,
            "r": 205.7745819091797,
            "b": 710.666748046875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "10",
      "text": "10"
    },
    {
      "self_ref": "#/texts/182",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 317.6997375488281,
            "t": 717.4136352539062,
            "r": 326.7662048339844,
            "b": 710.834716796875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "20",
      "text": "20"
    },
    {
      "self_ref": "#/texts/183",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 438.9523010253906,
            "t": 717.4136352539062,
            "r": 447.7602233886719,
            "b": 710.834716796875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "50",
      "text": "50"
    },
    {
      "self_ref": "#/texts/184",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 76.0815200805664,
            "t": 654.111572265625,
            "r": 84.76493072509766,
            "b": 647.5326538085938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "80",
      "text": "80"
    },
    {
      "self_ref": "#/texts/185",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 197.79904174804688,
            "t": 654.111572265625,
            "r": 210.4014892578125,
            "b": 647.5326538085938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "110",
      "text": "110"
    },
    {
      "self_ref": "#/texts/186",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 318.7330627441406,
            "t": 654.0515747070312,
            "r": 331.7043762207031,
            "b": 647.47265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "140",
      "text": "140"
    },
    {
      "self_ref": "#/texts/187",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 439.6371154785156,
            "t": 654.111572265625,
            "r": 452.6084289550781,
            "b": 647.5134887695312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "170",
      "text": "170"
    },
    {
      "self_ref": "#/texts/188",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 75.74166870117188,
            "t": 591.0374755859375,
            "r": 89.6323013305664,
            "b": 584.4585571289062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "200",
      "text": "200"
    },
    {
      "self_ref": "#/texts/189",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 196.7356719970703,
            "t": 590.7374877929688,
            "r": 210.62631225585938,
            "b": 584.1585693359375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "240",
      "text": "240"
    },
    {
      "self_ref": "#/texts/190",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 318.0052795410156,
            "t": 590.761474609375,
            "r": 331.7043762207031,
            "b": 584.1825561523438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "300",
      "text": "300"
    },
    {
      "self_ref": "#/texts/191",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 438.9093322753906,
            "t": 590.4314575195312,
            "r": 452.6084289550781,
            "b": 583.8525390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "360",
      "text": "360"
    },
    {
      "self_ref": "#/texts/192",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 75.69378662109375,
            "t": 526.5233154296875,
            "r": 89.6323013305664,
            "b": 519.9443969726562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "420",
      "text": "420"
    },
    {
      "self_ref": "#/texts/193",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 196.99423217773438,
            "t": 526.5233154296875,
            "r": 210.62631225585938,
            "b": 519.9443969726562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "500",
      "text": "500"
    },
    {
      "self_ref": "#/texts/194",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 318.0722961425781,
            "t": 526.5233154296875,
            "r": 331.7043762207031,
            "b": 519.9443969726562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "580",
      "text": "580"
    },
    {
      "self_ref": "#/texts/195",
      "parent": {
        "$ref": "#/pictures/4"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 438.928466796875,
            "t": 526.5653076171875,
            "r": 452.6084289550781,
            "b": 519.9863891601562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            3
          ]
        }
      ],
      "orig": "640",
      "text": "640"
    },
    {
      "self_ref": "#/texts/196",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.369056701660156,
            "t": 307.22674560546875,
            "r": 294.91705322265625,
            "b": 266.19927978515625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            249
          ]
        }
      ],
      "orig": "Figure 6. Comparison of SVD and ours. We compare our method with SVD for generating 26 frames on a zero-shot NuScenes [4] scene. In these moderately long-term videos, our method better preserves street lane details and car identity more effectively.",
      "text": "Figure 6. Comparison of SVD and ours. We compare our method with SVD for generating 26 frames on a zero-shot NuScenes [4] scene. In these moderately long-term videos, our method better preserves street lane details and car identity more effectively."
    },
    {
      "self_ref": "#/texts/197",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.41903305053711,
            "t": 360.18963623046875,
            "r": 65.85196685791016,
            "b": 333.22784423828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "(b) Ours",
      "text": "(b) Ours"
    },
    {
      "self_ref": "#/texts/198",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.41903305053711,
            "t": 417.6402587890625,
            "r": 65.85196685791016,
            "b": 390.71942138671875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "(a) SVD",
      "text": "(a) SVD"
    },
    {
      "self_ref": "#/texts/199",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 68.7547836303711,
            "t": 432.0091552734375,
            "r": 69.9757080078125,
            "b": 428.846923828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/200",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 68.7547836303711,
            "t": 373.77899169921875,
            "r": 69.9757080078125,
            "b": 370.61676025390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/201",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 126.2281265258789,
            "t": 432.0238037109375,
            "r": 130.65695190429688,
            "b": 428.81011962890625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "20",
      "text": "20"
    },
    {
      "self_ref": "#/texts/202",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 126.2281265258789,
            "t": 373.78485107421875,
            "r": 130.65695190429688,
            "b": 370.5711669921875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "20",
      "text": "20"
    },
    {
      "self_ref": "#/texts/203",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 183.38262939453125,
            "t": 432.0238037109375,
            "r": 187.5915985107422,
            "b": 428.81011962890625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "23",
      "text": "23"
    },
    {
      "self_ref": "#/texts/204",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 183.38262939453125,
            "t": 373.78485107421875,
            "r": 187.5915985107422,
            "b": 370.5711669921875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "23",
      "text": "23"
    },
    {
      "self_ref": "#/texts/205",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 240.60748291015625,
            "t": 432.0238037109375,
            "r": 245.02227783203125,
            "b": 428.81011962890625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "26",
      "text": "26"
    },
    {
      "self_ref": "#/texts/206",
      "parent": {
        "$ref": "#/pictures/5"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 240.689697265625,
            "t": 374.05926513671875,
            "r": 245.10757446289062,
            "b": 370.8407287597656,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "26",
      "text": "26"
    },
    {
      "self_ref": "#/texts/207",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.37747573852539,
            "t": 257.634765625,
            "r": 294.5472717285156,
            "b": 228.13221740722656,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            157
          ]
        }
      ],
      "orig": "Table 5. Performance comparison between our method and GPT-2. Our method not only improves efficiency but also produces better results on NuPlan [5] dataset.",
      "text": "Table 5. Performance comparison between our method and GPT-2. Our method not only improves efficiency but also produces better results on NuPlan [5] dataset."
    },
    {
      "self_ref": "#/texts/208",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 58.59145736694336,
            "t": 147.58045959472656,
            "r": 295.92864990234375,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            344
          ]
        }
      ],
      "orig": "cesses tokens sequentially across all frames during inference. GPT-2's serial token prediction slows down performance, significantly increasing computational burden and memory usage. As shown in Table 6, GPT-2's memory consumption grows quadratically with sequence length, making it inefficient for long sequences. In contrast, our method sepa-",
      "text": "cesses tokens sequentially across all frames during inference. GPT-2's serial token prediction slows down performance, significantly increasing computational burden and memory usage. As shown in Table 6, GPT-2's memory consumption grows quadratically with sequence length, making it inefficient for long sequences. In contrast, our method sepa-"
    },
    {
      "self_ref": "#/texts/209",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 317.12457275390625,
            "t": 431.8116760253906,
            "r": 554.5753173828125,
            "b": 401.72528076171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            152
          ]
        }
      ],
      "orig": "Table 6. Memory usage (GB) analysis of our method and GPT2. Our method consumes much lower GPU memory than GPT-2 structure. \"OOM\" means \"Out of Memory\".",
      "text": "Table 6. Memory usage (GB) analysis of our method and GPT2. Our method consumes much lower GPU memory than GPT-2 structure. \"OOM\" means \"Out of Memory\"."
    },
    {
      "self_ref": "#/texts/210",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 317.2804870605469,
            "t": 345.4494323730469,
            "r": 553.685791015625,
            "b": 264.7620849609375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            400
          ]
        }
      ],
      "orig": "s temporal and multimodal dependencies, allowing for more efficient representation and computation. As sequence lengths increase, our model maintains stable computational costs and memory usage, avoiding the sharp scaling seen in GPT-2. Moreover, our approach not only enhances efficiency but also improves result quality. As shown in Table 5, our model outperforms GPT-2 in FVD scores on NuPlan [5].",
      "text": "s temporal and multimodal dependencies, allowing for more efficient representation and computation. As sequence lengths increase, our model maintains stable computational costs and memory usage, avoiding the sharp scaling seen in GPT-2. Moreover, our approach not only enhances efficiency but also improves result quality. As shown in Table 5, our model outperforms GPT-2 in FVD scores on NuPlan [5]."
    },
    {
      "self_ref": "#/texts/211",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 317.51300048828125,
            "t": 251.6520538330078,
            "r": 478.3941650390625,
            "b": 243.17581176757812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            29
          ]
        }
      ],
      "orig": "5. Conclusion and Future Work",
      "text": "5. Conclusion and Future Work",
      "level": 1
    },
    {
      "self_ref": "#/texts/212",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 317.1044006347656,
            "t": 231.26646423339844,
            "r": 554.6361694335938,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            751
          ]
        }
      ],
      "orig": "In conclusion, DrivingWorld addressed the limitations of previous video generation models in autonomous driving by leveraging a GPT-style framework to produce longer, high-fidelity video predictions with improved generalization. Unlike traditional methods that struggled with coherence in long sequences or relied heavily on labeled data, DrivingWorld generated realistic, structured video sequences while enabling precise action control. Compared to the classic GPT structure, our proposed spatial-temporal GPT structure adopted next-state prediction strategy to model temporal coherence between consecutive frames and then applied next-token prediction strategy to capture spatial information within each frame. Looking ahead, we plan to incorporate",
      "text": "In conclusion, DrivingWorld addressed the limitations of previous video generation models in autonomous driving by leveraging a GPT-style framework to produce longer, high-fidelity video predictions with improved generalization. Unlike traditional methods that struggled with coherence in long sequences or relied heavily on labeled data, DrivingWorld generated realistic, structured video sequences while enabling precise action control. Compared to the classic GPT structure, our proposed spatial-temporal GPT structure adopted next-state prediction strategy to model temporal coherence between consecutive frames and then applied next-token prediction strategy to capture spatial information within each frame. Looking ahead, we plan to incorporate"
    },
    {
      "self_ref": "#/texts/213",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 304.06689453125,
            "t": 57.846717834472656,
            "r": 307.9423522949219,
            "b": 50.982486724853516,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "8",
      "text": "8"
    },
    {
      "self_ref": "#/texts/214",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 58.34623718261719,
            "t": 716.8414306640625,
            "r": 295.9354248046875,
            "b": 648.1001586914062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            347
          ]
        }
      ],
      "orig": "more multimodal information and integrate multiple view inputs. By fusing data from various modalities and viewpoints, we aim to improve action control and video generation accuracy, enhancing the model's ability to understand complex driving environments and further boosting the overall performance and reliability of autonomous driving systems.",
      "text": "more multimodal information and integrate multiple view inputs. By fusing data from various modalities and viewpoints, we aim to improve action control and video generation accuracy, enhancing the model's ability to understand complex driving environments and further boosting the overall performance and reliability of autonomous driving systems."
    },
    {
      "self_ref": "#/texts/215",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 58.23245620727539,
            "t": 645.1802368164062,
            "r": 294.34234619140625,
            "b": 612.234130859375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            154
          ]
        }
      ],
      "orig": "Acknowledgements. We sincerely thank Yang Hu for his discussion about code implementation at Horizon Robotics and Zhenhao Yang for meaningful suggestions.",
      "text": "Acknowledgements. We sincerely thank Yang Hu for his discussion about code implementation at Horizon Robotics and Zhenhao Yang for meaningful suggestions."
    },
    {
      "self_ref": "#/texts/216",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 303.8078918457031,
            "t": 57.846717834472656,
            "r": 308.08184814453125,
            "b": 50.90278625488281,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "9",
      "text": "9"
    },
    {
      "self_ref": "#/texts/217",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 58.81083679199219,
            "t": 718.2980346679688,
            "r": 113.70911407470703,
            "b": 709.881591796875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            10
          ]
        }
      ],
      "orig": "References",
      "text": "References",
      "level": 1
    },
    {
      "self_ref": "#/texts/218",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.77204513549805,
            "t": 698.22900390625,
            "r": 294.6697082519531,
            "b": 679.2003173828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            116
          ]
        }
      ],
      "orig": "[1] Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint arXiv:2008.05556, 2020. 2",
      "text": "[1] Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint arXiv:2008.05556, 2020. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/219",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.772037506103516,
            "t": 674.8240356445312,
            "r": 295.3647766113281,
            "b": 622.9193115234375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            286
          ]
        }
      ],
      "orig": "[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 6 , 1",
      "text": "[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/220",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.772037506103516,
            "t": 618.5430297851562,
            "r": 294.5917053222656,
            "b": 599.630859375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            96
          ]
        }
      ],
      "orig": "[3] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1",
      "text": "[3] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/221",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.77204513549805,
            "t": 595.1390380859375,
            "r": 296.08642578125,
            "b": 532.2753295898438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            330
          ]
        }
      ],
      "orig": "[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621-11631, 2020. 6 , 7 , 8 , 2",
      "text": "[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621-11631, 2020. 6 , 7 , 8 , 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/222",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.77204132080078,
            "t": 527.9439086914062,
            "r": 295.80377197265625,
            "b": 476.6846923828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            264
          ]
        }
      ],
      "orig": "[5] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: A closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. 6 , 7 , 8 , 1 , 2",
      "text": "[5] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: A closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. 6 , 7 , 8 , 1 , 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/223",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.77204513549805,
            "t": 471.62701416015625,
            "r": 295.80377197265625,
            "b": 430.67230224609375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            225
          ]
        }
      ],
      "orig": "[6] Christopher Diehl, Timo Sievernich, Martin Kruger, Frank \u00a8 Hoffmann, and Torsten Bertram. Umbrella: Uncertaintyaware model-based offline reinforcement learning leveraging planning. arXiv preprint arXiv:2111.11097, 2021. 2",
      "text": "[6] Christopher Diehl, Timo Sievernich, Martin Kruger, Frank \u00a8 Hoffmann, and Torsten Bertram. Umbrella: Uncertaintyaware model-based offline reinforcement learning leveraging planning. arXiv preprint arXiv:2111.11097, 2021. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/224",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.772037506103516,
            "t": 426.2960510253906,
            "r": 296.0926818847656,
            "b": 375.0816955566406,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            248
          ]
        }
      ],
      "orig": "[7] Christopher Diehl, Timo Sebastian Sievernich, Martin Kruger, Frank Hoffmann, and Torsten Bertram. Uncertainty-aware model-based offline reinforcement learning for automated driving. IEEE Robotics and Automation Letters, 8(2):1167- 1174, 2023. 2",
      "text": "[7] Christopher Diehl, Timo Sebastian Sievernich, Martin Kruger, Frank Hoffmann, and Torsten Bertram. Uncertainty-aware model-based offline reinforcement learning for automated driving. IEEE Robotics and Automation Letters, 8(2):1167- 1174, 2023. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/225",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.77204513549805,
            "t": 370.0150451660156,
            "r": 295.7902526855469,
            "b": 329.0693054199219,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            228
          ]
        }
      ],
      "orig": "[8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021. 2 , 3 , 7",
      "text": "[8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021. 2 , 3 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/226",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 63.77204513549805,
            "t": 324.69305419921875,
            "r": 295.367431640625,
            "b": 273.47869873046875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            250
          ]
        }
      ],
      "orig": "[9] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: A generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398 , 2024. 6 , 7",
      "text": "[9] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: A generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398 , 2024. 6 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/227",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 59.28904342651367,
            "t": 268.41204833984375,
            "r": 294.96099853515625,
            "b": 227.57388305664062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            218
          ]
        }
      ],
      "orig": "[10] Yanchen Guan, Haicheng Liao, Zhenning Li, Jia Hu, Runze Yuan, Yunjian Li, Guohui Zhang, and Chengzhong Xu. World models for autonomous driving: An initial survey. IEEE Transactions on Intelligent Vehicles, 2024. 2",
      "text": "[10] Yanchen Guan, Haicheng Liao, Zhenning Li, Jia Hu, Runze Yuan, Yunjian Li, Guohui Zhang, and Chengzhong Xu. World models for autonomous driving: An initial survey. IEEE Transactions on Intelligent Vehicles, 2024. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/228",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 59.28903579711914,
            "t": 223.09005737304688,
            "r": 295.80377197265625,
            "b": 193.102294921875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            172
          ]
        }
      ],
      "orig": "[11] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. 2",
      "text": "[11] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/229",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 59.28903579711914,
            "t": 188.72705078125,
            "r": 294.66839599609375,
            "b": 158.85585021972656,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            156
          ]
        }
      ],
      "orig": "[12] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 2",
      "text": "[12] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/230",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 59.28904342651367,
            "t": 154.3640594482422,
            "r": 295.6975402832031,
            "b": 124.49285125732422,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            160
          ]
        }
      ],
      "orig": "[13] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. 2",
      "text": "[13] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/231",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 59.28903579711914,
            "t": 120.00105285644531,
            "r": 296.0889892578125,
            "b": 79.05429077148438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            225
          ]
        }
      ],
      "orig": "[14] Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, and Sergey Tulyakov. Show me what and tell me how: Video synthesis via multimodal conditioning. In Proceedings of",
      "text": "[14] Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, and Sergey Tulyakov. Show me what and tell me how: Video synthesis via multimodal conditioning. In Proceedings of",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/232",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 302.1248474121094,
            "t": 57.846717834472656,
            "r": 310.7425231933594,
            "b": 50.982486724853516,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "10",
      "text": "10"
    },
    {
      "self_ref": "#/texts/233",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 336.78338623046875,
            "t": 716.1610107421875,
            "r": 553.2640380859375,
            "b": 697.13232421875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            92
          ]
        }
      ],
      "orig": "the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3615-3625, 2022. 3",
      "text": "the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3615-3625, 2022. 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/234",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 693.9758911132812,
            "r": 554.5546875,
            "b": 654.74169921875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            189
          ]
        }
      ],
      "orig": "[15] Mikael Henaff, Alfredo Canziani, and Yann LeCun. Modelpredictive policy learning with uncertainty regularization for driving in dense traffic. arXiv preprint arXiv:1901.02705 , 2019. 2",
      "text": "[15] Mikael Henaff, Alfredo Canziani, and Yann LeCun. Modelpredictive policy learning with uncertainty regularization for driving in dense traffic. arXiv preprint arXiv:1901.02705 , 2019. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/235",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 649.791015625,
            "r": 554.4362182617188,
            "b": 619.9108276367188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            155
          ]
        }
      ],
      "orig": "[16] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. 6",
      "text": "[16] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. 6",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/236",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 616.593017578125,
            "r": 554.5538330078125,
            "b": 575.6463012695312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            224
          ]
        }
      ],
      "orig": "[17] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 2 , 6",
      "text": "[17] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 2 , 6",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/237",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 572.4440307617188,
            "r": 554.55078125,
            "b": 531.498291015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            239
          ]
        }
      ],
      "orig": "[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125-1134, 2017. 2",
      "text": "[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125-1134, 2017. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/238",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 528.2960205078125,
            "r": 554.5707397460938,
            "b": 487.46588134765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            205
          ]
        }
      ],
      "orig": "[19] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang. Adriveri: A general world model for autonomous driving. arXiv preprint arXiv:2311.13549, 2023. 6",
      "text": "[19] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang. Adriveri: A general world model for autonomous driving. arXiv preprint arXiv:2311.13549, 2023. 6",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/239",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 484.1470642089844,
            "r": 554.1331787109375,
            "b": 434.0714111328125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            247
          ]
        }
      ],
      "orig": "[20] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards a controllable high-quality neural simulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5820-5829, 2021. 6",
      "text": "[20] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards a controllable high-quality neural simulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5820-5829, 2021. 6",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/240",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 429.0400390625,
            "r": 554.465576171875,
            "b": 399.74267578125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            125
          ]
        }
      ],
      "orig": "[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Int. Conf. Learn. Represent., 2015. 1 , 2",
      "text": "[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Int. Conf. Learn. Represent., 2015. 1 , 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/241",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 395.8500671386719,
            "r": 554.5537719726562,
            "b": 333.0849304199219,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            362
          ]
        }
      ],
      "orig": "[22] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):1956-1981, 2020. 6 , 1",
      "text": "[22] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):1956-1981, 2020. 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/242",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 329.7840576171875,
            "r": 554.552734375,
            "b": 278.5686950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            243
          ]
        }
      ],
      "orig": "[23] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and MingHsuan Yang. Fast and accurate image super-resolution with deep laplacian pyramid networks. IEEE transactions on pattern analysis and machine intelligence, 41(11):2599-2613, 2018. 6 , 1",
      "text": "[23] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and MingHsuan Yang. Fast and accurate image super-resolution with deep laplacian pyramid networks. IEEE transactions on pattern analysis and machine intelligence, 41(11):2599-2613, 2018. 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/243",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 274.67706298828125,
            "r": 554.5537719726562,
            "b": 246.5184326171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            124
          ]
        }
      ],
      "orig": "[24] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1-62, 2022. 2",
      "text": "[24] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1-62, 2022. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/244",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 241.53187561035156,
            "r": 554.12060546875,
            "b": 178.623291015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            335
          ]
        }
      ],
      "orig": "[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence ' ' Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014. 6 , 1",
      "text": "[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence ' ' Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014. 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/245",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 175.4300079345703,
            "r": 554.5538330078125,
            "b": 147.37899780273438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            124
          ]
        }
      ],
      "orig": "[26] Ilya Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017. 1 , 2",
      "text": "[26] Ilya Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017. 1 , 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/246",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 142.23104858398438,
            "r": 554.5569458007812,
            "b": 101.40185546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            208
          ]
        }
      ],
      "orig": "[27] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li Zhang. WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation. arXiv preprint arXiv:2312.02934, 2023. 2 , 6 , 7",
      "text": "[27] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li Zhang. WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation. arXiv preprint arXiv:2312.02934, 2023. 2 , 6 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/247",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 318.0390319824219,
            "t": 98.08305358886719,
            "r": 554.5576782226562,
            "b": 79.05429077148438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            87
          ]
        }
      ],
      "orig": "[28] Alec Radford. Improving language understanding by generative pre-training. 2018. 1",
      "text": "[28] Alec Radford. Improving language understanding by generative pre-training. 2018. 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/248",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 716.1610107421875,
            "r": 295.81298828125,
            "b": 677.04443359375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            186
          ]
        }
      ],
      "orig": "[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 , 4 , 7 , 8",
      "text": "[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 , 4 , 7 , 8",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/249",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 671.14599609375,
            "r": 295.7145080566406,
            "b": 652.2338256835938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            113
          ]
        }
      ],
      "orig": "[30] Eder Santana and George Hotz. Learning a Driving Simulator. arXiv preprint arXiv:1608.01230, 2016. 2 , 6 , 7",
      "text": "[30] Eder Santana and George Hotz. Learning a Driving Simulator. arXiv preprint arXiv:1608.01230, 2016. 2 , 6 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/250",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28903579711914,
            "t": 648.0480346679688,
            "r": 295.8038024902344,
            "b": 596.143310546875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            268
          ]
        }
      ],
      "orig": "[31] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3626-3636, 2022. 2",
      "text": "[31] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3626-3636, 2022. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/251",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 592.0740356445312,
            "r": 295.3689270019531,
            "b": 552.9564208984375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            178
          ]
        }
      ],
      "orig": "[32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 6",
      "text": "[32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 6",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/252",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 547.0580444335938,
            "r": 294.5875549316406,
            "b": 506.2278747558594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            226
          ]
        }
      ],
      "orig": "[33] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2 , 3 , 5 , 7 , 1",
      "text": "[33] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2 , 3 , 5 , 7 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/253",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 502.0430603027344,
            "r": 295.8038024902344,
            "b": 461.78668212890625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            198
          ]
        }
      ],
      "orig": "[34] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 2 , 7",
      "text": "[34] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 2 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/254",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 457.0718688964844,
            "r": 295.7145080566406,
            "b": 416.1968688964844,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            221
          ]
        }
      ],
      "orig": "[35] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069, 2021. 2",
      "text": "[35] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069, 2021. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/255",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28903579711914,
            "t": 412.0210266113281,
            "r": 295.7811279296875,
            "b": 382.13189697265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            162
          ]
        }
      ],
      "orig": "[36] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 , 3 , 6 , 1",
      "text": "[36] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 , 3 , 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/256",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 377.9550476074219,
            "r": 295.81024169921875,
            "b": 337.6986999511719,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            206
          ]
        }
      ],
      "orig": "[37] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddriven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. 2 , 6 , 7",
      "text": "[37] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddriven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. 2 , 6 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/257",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 332.9400634765625,
            "r": 295.8008728027344,
            "b": 281.14190673828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            244
          ]
        }
      ],
      "orig": "[38] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 6 , 7",
      "text": "[38] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 6 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/258",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.289039611816406,
            "t": 276.9650573730469,
            "r": 295.7992858886719,
            "b": 214.7917022705078,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            302
          ]
        }
      ],
      "orig": "[39] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14749-14759, 2024. 2",
      "text": "[39] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14749-14759, 2024. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/259",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 210.0320587158203,
            "r": 295.367431640625,
            "b": 169.08529663085938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            206
          ]
        }
      ],
      "orig": "[40] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on robot learning , pages 2226-2240. PMLR, 2023. 2",
      "text": "[40] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on robot learning , pages 2226-2240. PMLR, 2023. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/260",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28904342651367,
            "t": 165.01605224609375,
            "r": 295.8199462890625,
            "b": 135.1448516845703,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            166
          ]
        }
      ],
      "orig": "[41] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3",
      "text": "[41] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/261",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 59.28903579711914,
            "t": 130.96005249023438,
            "r": 295.7098693847656,
            "b": 79.15292358398438,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            280
          ]
        }
      ],
      "orig": "[42] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang Li. Generalized Predictive Model for Autonomous Driving. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 6 , 7",
      "text": "[42] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang Li. Generalized Predictive Model for Autonomous Driving. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 6 , 7",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/262",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 302.1248474121094,
            "t": 57.846717834472656,
            "r": 309.9255676269531,
            "b": 51.11199951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            2
          ]
        }
      ],
      "orig": "11",
      "text": "11"
    },
    {
      "self_ref": "#/texts/263",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 318.0390319824219,
            "t": 716.1610107421875,
            "r": 554.1251220703125,
            "b": 675.21533203125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            227
          ]
        }
      ],
      "orig": "[43] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2",
      "text": "[43] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/264",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 318.0390319824219,
            "t": 671.3290405273438,
            "r": 554.4666748046875,
            "b": 630.4998168945312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            214
          ]
        }
      ],
      "orig": "[44] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. 2",
      "text": "[44] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. 2",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/265",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 318.0390319824219,
            "t": 626.5428466796875,
            "r": 553.4197387695312,
            "b": 585.5513305664062,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            226
          ]
        }
      ],
      "orig": "[45] Qihang Zhang, Zhenghao Peng, and Bolei Zhou. Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining. In European Conference on Computer Vision, pages 111-128. Springer, 2022. 6 , 1",
      "text": "[45] Qihang Zhang, Zhenghao Peng, and Bolei Zhou. Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining. In European Conference on Computer Vision, pages 111-128. Springer, 2022. 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/266",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 318.0390319824219,
            "t": 581.6749877929688,
            "r": 554.1341552734375,
            "b": 530.45068359375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            267
          ]
        }
      ],
      "orig": "[46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586-595, 2018. 2 , 6 , 1",
      "text": "[46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586-595, 2018. 2 , 6 , 1",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/267",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 318.0390319824219,
            "t": 525.875,
            "r": 554.5307006835938,
            "b": 485.035888671875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            216
          ]
        }
      ],
      "orig": "[47] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:23412-23425, 2022. 2 , 3",
      "text": "[47] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:23412-23425, 2022. 2 , 3",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/268",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.69683074951172,
            "t": 720.0292358398438,
            "r": 553.541259765625,
            "b": 707.175048828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            75
          ]
        }
      ],
      "orig": "DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT",
      "text": "DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT",
      "level": 1
    },
    {
      "self_ref": "#/texts/269",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 237.0745391845703,
            "t": 694.8084716796875,
            "r": 375.2284240722656,
            "b": 681.8969116210938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            22
          ]
        }
      ],
      "orig": "Supplementary Material",
      "text": "Supplementary Material",
      "level": 1
    },
    {
      "self_ref": "#/texts/270",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.35021209716797,
            "t": 664.3994140625,
            "r": 295.4383544921875,
            "b": 643.4880981445312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            66
          ]
        }
      ],
      "orig": "We provide more details of DrivingWorld. Specifically, we provide:",
      "text": "We provide more details of DrivingWorld. Specifically, we provide:"
    },
    {
      "self_ref": "#/texts/271",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 70.23650360107422,
            "t": 638.0073852539062,
            "r": 165.06053161621094,
            "b": 629.0410766601562,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            25
          ]
        }
      ],
      "orig": "\u00b7 Societal Impact in \u00a76 .",
      "text": "\u00b7 Societal Impact in \u00a76 .",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/272",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 70.23650360107422,
            "t": 626.04248046875,
            "r": 174.02685546875,
            "b": 617.7735595703125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            23
          ]
        }
      ],
      "orig": "\u00b7 SVD Refinement in \u00a77;",
      "text": "\u00b7 SVD Refinement in \u00a77;",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/273",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 70.23650360107422,
            "t": 614.0874633789062,
            "r": 152.20877075195312,
            "b": 605.8185424804688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "\u00b7 Private Data in \u00a78;",
      "text": "\u00b7 Private Data in \u00a78;",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/274",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 70.23650360107422,
            "t": 602.1324462890625,
            "r": 191.9296417236328,
            "b": 593.1661376953125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            30
          ]
        }
      ],
      "orig": "\u00b7 More Training Details in \u00a79;",
      "text": "\u00b7 More Training Details in \u00a79;",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/275",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "label": "list_item",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 70.23650360107422,
            "t": 590.1764526367188,
            "r": 198.93336486816406,
            "b": 581.9075317382812,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            31
          ]
        }
      ],
      "orig": "\u00b7 More Ablation Studies in \u00a710;",
      "text": "\u00b7 More Ablation Studies in \u00a710;",
      "enumerated": false,
      "marker": "-"
    },
    {
      "self_ref": "#/texts/276",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.834747314453125,
            "t": 566.9860229492188,
            "r": 149.80186462402344,
            "b": 556.274169921875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            18
          ]
        }
      ],
      "orig": "6. Societal Impact",
      "text": "6. Societal Impact",
      "level": 1
    },
    {
      "self_ref": "#/texts/277",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.35697937011719,
            "t": 546.3394165039062,
            "r": 294.45416259765625,
            "b": 513.463134765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            151
          ]
        }
      ],
      "orig": "This work potentially benefits autonomous driving and video generation fields. The authors believe that this work has small potential negative impacts.",
      "text": "This work potentially benefits autonomous driving and video generation fields. The authors believe that this work has small potential negative impacts."
    },
    {
      "self_ref": "#/texts/278",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.70323944091797,
            "t": 499.239013671875,
            "r": 155.77944946289062,
            "b": 490.7508239746094,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            17
          ]
        }
      ],
      "orig": "7. SVD Refinement",
      "text": "7. SVD Refinement",
      "level": 1
    },
    {
      "self_ref": "#/texts/279",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.44407653808594,
            "t": 478.5914306640625,
            "r": 295.934814453125,
            "b": 373.9841003417969,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            482
          ]
        }
      ],
      "orig": "Using the autoregressive process, future long-duration videos and ego states can be predicted. Due to limited GPU memory, each image is restricted to a resolution of 256 \u00d7 512. However, applying interpolation methods such as bicubic to upscale to higher resolutions (i.e., 512 \u00d7 1024) often yields suboptimal results. By leveraging existing opensource weights, we can easily fine-tune a SVD [2] model to refine the generated images and extend them to higher resolutions effectively.",
      "text": "Using the autoregressive process, future long-duration videos and ego states can be predicted. Due to limited GPU memory, each image is restricted to a resolution of 256 \u00d7 512. However, applying interpolation methods such as bicubic to upscale to higher resolutions (i.e., 512 \u00d7 1024) often yields suboptimal results. By leveraging existing opensource weights, we can easily fine-tune a SVD [2] model to refine the generated images and extend them to higher resolutions effectively."
    },
    {
      "self_ref": "#/texts/280",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.34602737426758,
            "t": 370.7454528808594,
            "r": 295.79248046875,
            "b": 206.36212158203125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            797
          ]
        }
      ],
      "orig": "The input to SVD refinement is low-resolution image sequence generated by DrivingWorld, and the output is the refined high-resolution images. We use the same dataset as DrivingWorld to train the SVD model. To refine this model, we simulate the videos generated by DrivingWorld by replacing p percent of tokens in the image sequence with those processed by the temporal-aware VQVAE. The images decoded by the VQVAE are used as input, while the original images are treated as labels. In our training, p is set to 15%. We employ the AdamW optimizer [21 , 26] with no weight decay, (\u03b21, \u03b22) set to (0 . 9 , 0 . 999), and a learning rate that warms up over 1000 steps to a maximum of 1 \u00d7 10 - 5 . The image sequence is set to 25, and the training process converges within one day on 8 NVIDIA A100 GPUs.",
      "text": "The input to SVD refinement is low-resolution image sequence generated by DrivingWorld, and the output is the refined high-resolution images. We use the same dataset as DrivingWorld to train the SVD model. To refine this model, we simulate the videos generated by DrivingWorld by replacing p percent of tokens in the image sequence with those processed by the temporal-aware VQVAE. The images decoded by the VQVAE are used as input, while the original images are treated as labels. In our training, p is set to 15%. We employ the AdamW optimizer [21 , 26] with no weight decay, (\u03b21, \u03b22) set to (0 . 9 , 0 . 999), and a learning rate that warms up over 1000 steps to a maximum of 1 \u00d7 10 - 5 . The image sequence is set to 25, and the training process converges within one day on 8 NVIDIA A100 GPUs."
    },
    {
      "self_ref": "#/texts/281",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.834747314453125,
            "t": 192.1260528564453,
            "r": 134.81002807617188,
            "b": 183.7095947265625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            15
          ]
        }
      ],
      "orig": "8. Private Data",
      "text": "8. Private Data",
      "level": 1
    },
    {
      "self_ref": "#/texts/282",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 58.55080795288086,
            "t": 171.49046325683594,
            "r": 295.93280029296875,
            "b": 78.84807586669922,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            466
          ]
        }
      ],
      "orig": "Our private dataset is an extensive collection of 3 , 336 hours of human driving data gathered from four major cities in China: Beijing, Nanjing, Hefei, and Tianjin. This dataset is automatically annotated using a state-of-the-art offline perception system. Approximately 1 , 668 hours of data originate from Beijing, with the remaining hours evenly distributed among the other three cities. Approximately twothirds of the scenarios take place in urban environments,",
      "text": "Our private dataset is an extensive collection of 3 , 336 hours of human driving data gathered from four major cities in China: Beijing, Nanjing, Hefei, and Tianjin. This dataset is automatically annotated using a state-of-the-art offline perception system. Approximately 1 , 668 hours of data originate from Beijing, with the remaining hours evenly distributed among the other three cities. Approximately twothirds of the scenarios take place in urban environments,"
    },
    {
      "self_ref": "#/texts/283",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 317.1002197265625,
            "t": 664.3994140625,
            "r": 553.6898193359375,
            "b": 633.5554809570312,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            136
          ]
        }
      ],
      "orig": "with the remaining one-third covering highway and rural roads. All data are collected during daytime and under clear weather conditions.",
      "text": "with the remaining one-third covering highway and rural roads. All data are collected during daytime and under clear weather conditions."
    },
    {
      "self_ref": "#/texts/284",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 317.0703125,
            "t": 628.534423828125,
            "r": 554.663818359375,
            "b": 525.95849609375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            524
          ]
        }
      ],
      "orig": "The original images are captured at a resolution of (2160 , 3840) and subsequently downsampled and centercropped to (512 , 1024). Six different views (front, leftfront, left-rear, rear, right-front, and right-rear) are initially recorded. But for the current version, we only use the front view. For each frame, we represent the center position (tx, ty, tz) and a quaternion (qx, qy, qz, qw) of the vehicle in world coordinate system, from which we can compute relative locations and orientations based on the initial frame.",
      "text": "The original images are captured at a resolution of (2160 , 3840) and subsequently downsampled and centercropped to (512 , 1024). Six different views (front, leftfront, left-rear, rear, right-front, and right-rear) are initially recorded. But for the current version, we only use the front view. For each frame, we represent the center position (tx, ty, tz) and a quaternion (qx, qy, qz, qw) of the vehicle in world coordinate system, from which we can compute relative locations and orientations based on the initial frame."
    },
    {
      "self_ref": "#/texts/285",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 317.5608215332031,
            "t": 511.0770568847656,
            "r": 442.00250244140625,
            "b": 500.3652038574219,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            24
          ]
        }
      ],
      "orig": "9. More Training Details",
      "text": "9. More Training Details",
      "level": 1
    },
    {
      "self_ref": "#/texts/286",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 316.8327331542969,
            "t": 490.7711486816406,
            "r": 554.6680297851562,
            "b": 386.0841064453125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            516
          ]
        }
      ],
      "orig": "Tokenizer and Decoder. The temporal-aware VQVAE has 70M parameters in total. The images are with size of 256 \u00d7 512 and tokenized into 512 tokens. Considering 2 tokens for orientation and location respectively, each state consists of 514 tokens. The size of adopted codebook is set to 16 , 384 . We employ the AdamW optimizer [21 , 26] with no weight decay, and (\u03b21, \u03b22) is set to (0 . 9 , 0 . 95). Following LlamaGen [33], we also use a fixed learning rate but with a smaller value, which is set to only 1 \u00d7 10 - 5 .",
      "text": "Tokenizer and Decoder. The temporal-aware VQVAE has 70M parameters in total. The images are with size of 256 \u00d7 512 and tokenized into 512 tokens. Considering 2 tokens for orientation and location respectively, each state consists of 514 tokens. The size of adopted codebook is set to 16 , 384 . We employ the AdamW optimizer [21 , 26] with no weight decay, and (\u03b21, \u03b22) is set to (0 . 9 , 0 . 95). Following LlamaGen [33], we also use a fixed learning rate but with a smaller value, which is set to only 1 \u00d7 10 - 5 ."
    },
    {
      "self_ref": "#/texts/287",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 316.8308410644531,
            "t": 383.0954284667969,
            "r": 554.6624755859375,
            "b": 231.2847900390625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            743
          ]
        }
      ],
      "orig": "We train the temporal-aware VQVAE model in two stages. In the first stage, to mitigate instability at the early stages of training, we only focus on teaching the model to process spatial information in images. For this stage, we use the OpenImages [22] and COCO [25] datasets, consisting of a total of 6M single-frame images. In the second stage, to enable the model to capture sufficient temporal information, we train it using sequences of 15 consecutive video frames. For this stage, we use data from YouTubeDV [45] and NuPlan [5], totaling 9M videos, each consisting of 15 frames. We train the temporal-aware VQVAE using a combination of three loss functions: charbonnier loss [23], perceptual loss from LPIPS [46], and codebook loss [36].",
      "text": "We train the temporal-aware VQVAE model in two stages. In the first stage, to mitigate instability at the early stages of training, we only focus on teaching the model to process spatial information in images. For this stage, we use the OpenImages [22] and COCO [25] datasets, consisting of a total of 6M single-frame images. In the second stage, to enable the model to capture sufficient temporal information, we train it using sequences of 15 consecutive video frames. For this stage, we use data from YouTubeDV [45] and NuPlan [5], totaling 9M videos, each consisting of 15 frames. We train the temporal-aware VQVAE using a combination of three loss functions: charbonnier loss [23], perceptual loss from LPIPS [46], and codebook loss [36]."
    },
    {
      "self_ref": "#/texts/288",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "formula",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 538.0482177734375,
            "t": 207.28671264648438,
            "r": 553.6895141601562,
            "b": 198.798583984375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            4
          ]
        }
      ],
      "orig": "(10)",
      "text": "(10)"
    },
    {
      "self_ref": "#/texts/289",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 317.1044006347656,
            "t": 195.51104736328125,
            "r": 554.18896484375,
            "b": 174.4791259765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            75
          ]
        }
      ],
      "orig": "where the values of \u03bb 1, \u03bb 2 , and \u03bb3 are set to 3, 1, and 1, respectively.",
      "text": "where the values of \u03bb 1, \u03bb 2 , and \u03bb3 are set to 3, 1, and 1, respectively."
    },
    {
      "self_ref": "#/texts/290",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 317.41943359375,
            "t": 171.49046325683594,
            "r": 553.6904296875,
            "b": 126.65911865234375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            191
          ]
        }
      ],
      "orig": "The model is trained in two stages, each running for 500K steps with a total batch size of 128, distributed across 32 NVIDIA 4090 GPUs. The entire training process takes approximately 7 days.",
      "text": "The model is trained in two stages, each running for 500K steps with a total batch size of 128, distributed across 32 NVIDIA 4090 GPUs. The entire training process takes approximately 7 days."
    },
    {
      "self_ref": "#/texts/291",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 316.9388427734375,
            "t": 123.67045593261719,
            "r": 554.5620727539062,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            237
          ]
        }
      ],
      "orig": "World Model. The world model has 1B parameters in total. The first 15 frames serve as conditional inputs, with the final frame used for supervision. With 514 tokens per image, the sequence consists of a total of 7 , 710 tokens. We employ",
      "text": "World Model. The world model has 1B parameters in total. The first 15 frames serve as conditional inputs, with the final frame used for supervision. With 514 tokens per image, the sequence consists of a total of 7 , 710 tokens. We employ"
    },
    {
      "self_ref": "#/texts/292",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 304.6148376464844,
            "t": 57.846717834472656,
            "r": 307.43426513671875,
            "b": 51.11199951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "1",
      "text": "1"
    },
    {
      "self_ref": "#/texts/293",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.37747573852539,
            "t": 718.5247802734375,
            "r": 295.7724609375,
            "b": 699.42431640625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            118
          ]
        }
      ],
      "orig": "Table 7. Comparison of different condition frames. DrivingWorld generates better videos when conditioning more frames.",
      "text": "Table 7. Comparison of different condition frames. DrivingWorld generates better videos when conditioning more frames."
    },
    {
      "self_ref": "#/texts/294",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.32029724121094,
            "t": 611.1663818359375,
            "r": 294.6602783203125,
            "b": 578.2801513671875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            155
          ]
        }
      ],
      "orig": "the AdamW optimizer [21 , 26] with no weight decay, and (\u03b21, \u03b22) is set to (0 . 9 , 0 . 95). We use a fixed learning rate which is set to only 1 \u00d7 10 - 4 .",
      "text": "the AdamW optimizer [21 , 26] with no weight decay, and (\u03b21, \u03b22) is set to (0 . 9 , 0 . 95). We use a fixed learning rate which is set to only 1 \u00d7 10 - 4 ."
    },
    {
      "self_ref": "#/texts/295",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.29043960571289,
            "t": 575.2674560546875,
            "r": 295.916748046875,
            "b": 508.5574951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            297
          ]
        }
      ],
      "orig": "The world model is trained on over 3456 hours of human driving data. 120 hours come from the publicly available NuPlan [5] dataset, and 3336 hours consist of private data. Training is conducted over 12 days, spanning 450K iterations with a batch size of 64, distributed across 64 NVIDIA A100 GPUs.",
      "text": "The world model is trained on over 3456 hours of human driving data. 120 hours come from the publicly available NuPlan [5] dataset, and 3336 hours consist of private data. Training is conducted over 12 days, spanning 450K iterations with a batch size of 64, distributed across 64 NVIDIA A100 GPUs."
    },
    {
      "self_ref": "#/texts/296",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 59.2770881652832,
            "t": 492.9770202636719,
            "r": 191.45379638671875,
            "b": 484.48883056640625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            25
          ]
        }
      ],
      "orig": "10. More Ablation Studies",
      "text": "10. More Ablation Studies",
      "level": 1
    },
    {
      "self_ref": "#/texts/297",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.08280944824219,
            "t": 472.5554504394531,
            "r": 295.7961120605469,
            "b": 381.85577392578125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            423
          ]
        }
      ],
      "orig": "Due to the prolonged training time and computational costs, we experiment on a smaller dataset for the ablation study. We extract 12 hours of video data from the NuPlan [5] dataset for training, and select 20 videos from NuPlan [5] and Nuscenes [4] test sets to create our testing data. All ablations are conducted on 32 NVIDIA A100 GPUs with a total batch size of 32. Each model is trained from scratch for 50K iterations.",
      "text": "Due to the prolonged training time and computational costs, we experiment on a smaller dataset for the ablation study. We extract 12 hours of video data from the NuPlan [5] dataset for training, and select 20 videos from NuPlan [5] and Nuscenes [4] test sets to create our testing data. All ablations are conducted on 32 NVIDIA A100 GPUs with a total batch size of 32. Each model is trained from scratch for 50K iterations."
    },
    {
      "self_ref": "#/texts/298",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 59.21232986450195,
            "t": 369.6396179199219,
            "r": 212.92185974121094,
            "b": 361.8697509765625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            32
          ]
        }
      ],
      "orig": "10.1. Different Condition Frames",
      "text": "10.1. Different Condition Frames",
      "level": 1
    },
    {
      "self_ref": "#/texts/299",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.36256790161133,
            "t": 351.9104309082031,
            "r": 295.91131591796875,
            "b": 223.403076171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            594
          ]
        }
      ],
      "orig": "To investigate the effect of the number of condition frames on model performance, we conduct a series of experiments by gradually increasing the length of the condition frames used during training and inference. As shown in Table 7 , the model consistently improves as the number of condition frames increases. Specifically, when fewer condition frames are used, the model struggles to capture long-term dependencies. In contrast, with longer condition frames, the model has more temporal context to work with, allowing it to better understand the environment and generate more precise outputs.",
      "text": "To investigate the effect of the number of condition frames on model performance, we conduct a series of experiments by gradually increasing the length of the condition frames used during training and inference. As shown in Table 7 , the model consistently improves as the number of condition frames increases. Specifically, when fewer condition frames are used, the model struggles to capture long-term dependencies. In contrast, with longer condition frames, the model has more temporal context to work with, allowing it to better understand the environment and generate more precise outputs."
    },
    {
      "self_ref": "#/texts/300",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 59.21232986450195,
            "t": 213.1295928955078,
            "r": 294.24786376953125,
            "b": 193.4595184326172,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            52
          ]
        }
      ],
      "orig": "10.2. Impact of Internal-state Autoregressive Module",
      "text": "10.2. Impact of Internal-state Autoregressive Module",
      "level": 1
    },
    {
      "self_ref": "#/texts/301",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.34623718261719,
            "t": 183.44546508789062,
            "r": 295.8106994628906,
            "b": 78.83811950683594,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            523
          ]
        }
      ],
      "orig": "To assess the impact of the final internal-state autoregressive (AR) module on our DrivingWorld's overall performance, we perform an ablation study by removing this module from the model structure. Thus future state's tokens are predicted simultaneously. The experimental results, as summarized in Table 8, indicate that the absence of the AR module leads to a noticeable decrease in performance across FVD metric. Note that \"w/o AR\" and \"Ours\" have comparable model size. Specifically, removing the AR module results in an",
      "text": "To assess the impact of the final internal-state autoregressive (AR) module on our DrivingWorld's overall performance, we perform an ablation study by removing this module from the model structure. Thus future state's tokens are predicted simultaneously. The experimental results, as summarized in Table 8, indicate that the absence of the AR module leads to a noticeable decrease in performance across FVD metric. Note that \"w/o AR\" and \"Ours\" have comparable model size. Specifically, removing the AR module results in an"
    },
    {
      "self_ref": "#/texts/302",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "page_footer",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 303.8078918457031,
            "t": 57.846717834472656,
            "r": 308.2412414550781,
            "b": 51.11199951171875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            1
          ]
        }
      ],
      "orig": "2",
      "text": "2"
    },
    {
      "self_ref": "#/texts/303",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 316.4502868652344,
            "t": 718.5247802734375,
            "r": 554.563232421875,
            "b": 667.1032104492188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            315
          ]
        }
      ],
      "orig": "Table 8. Comparison of 'w/ or 'w/o internal-state autoregressive module. \" w/o AR\" removes the internal-state autoregressive module and generates all next-state tokens simultaneously, while \"Ours\" autoregressively generates next-state tokens, which have much lower FVD error on NuPlan [5] and Nuscenes [4] datasets.",
      "text": "Table 8. Comparison of 'w/ or 'w/o internal-state autoregressive module. \" w/o AR\" removes the internal-state autoregressive module and generates all next-state tokens simultaneously, while \"Ours\" autoregressively generates next-state tokens, which have much lower FVD error on NuPlan [5] and Nuscenes [4] datasets."
    },
    {
      "self_ref": "#/texts/304",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "caption",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 317.1274719238281,
            "t": 599.0947875976562,
            "r": 553.5098266601562,
            "b": 569.5822143554688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            181
          ]
        }
      ],
      "orig": "Table 9. Scaling law of our model. We compare three different model sizes (i.e. 10M, 100M, 1B). Larger model can generate much better videos on NuPlan [5] and Nuscenes [4] datasets.",
      "text": "Table 9. Scaling law of our model. We compare three different model sizes (i.e. 10M, 100M, 1B). Larger model can generate much better videos on NuPlan [5] and Nuscenes [4] datasets."
    },
    {
      "self_ref": "#/texts/305",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 317.3813171386719,
            "t": 483.4434509277344,
            "r": 553.300537109375,
            "b": 438.6120910644531,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            199
          ]
        }
      ],
      "orig": "increase from 18% to 71% in FVD metric, which suggests that the module plays a crucial role in capturing sequential dependencies and refining the final output predictions in the long-term generation.",
      "text": "increase from 18% to 71% in FVD metric, which suggests that the module plays a crucial role in capturing sequential dependencies and refining the final output predictions in the long-term generation."
    },
    {
      "self_ref": "#/texts/306",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "section_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 317.96234130859375,
            "t": 428.43255615234375,
            "r": 501.48004150390625,
            "b": 418.6024169921875,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            37
          ]
        }
      ],
      "orig": "10.3. Scaling Law of Our DrivingWorld",
      "text": "10.3. Scaling Law of Our DrivingWorld",
      "level": 1
    },
    {
      "self_ref": "#/texts/307",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "text",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 317.1090087890625,
            "t": 410.7164306640625,
            "r": 554.6848754882812,
            "b": 318.0641174316406,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            427
          ]
        }
      ],
      "orig": "To investigate the scaling law of our model, we conducted a series of ablation experiments by progressively scaling up the number of parameters in the model. As shown in Table 9 , increasing the model size consistently leads to improved performance. In smaller models, the limited capacity hinders the ability to fully capture the complexity of the data, resulting in suboptimal performance, especially on long-term generation.",
      "text": "To investigate the scaling law of our model, we conducted a series of ablation experiments by progressively scaling up the number of parameters in the model. As shown in Table 9 , increasing the model size consistently leads to improved performance. In smaller models, the limited capacity hinders the ability to fully capture the complexity of the data, resulting in suboptimal performance, especially on long-term generation."
    }
  ],
  "pictures": [
    {
      "self_ref": "#/pictures/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/6"
        },
        {
          "$ref": "#/texts/7"
        },
        {
          "$ref": "#/texts/8"
        },
        {
          "$ref": "#/texts/9"
        },
        {
          "$ref": "#/texts/10"
        },
        {
          "$ref": "#/texts/11"
        },
        {
          "$ref": "#/texts/12"
        },
        {
          "$ref": "#/texts/13"
        },
        {
          "$ref": "#/texts/14"
        },
        {
          "$ref": "#/texts/15"
        }
      ],
      "label": "picture",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 59.9542350769043,
            "t": 563.3037719726562,
            "r": 550.9872436523438,
            "b": 417.0596618652344,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            367
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/5"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": []
    },
    {
      "self_ref": "#/pictures/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/52"
        },
        {
          "$ref": "#/texts/53"
        },
        {
          "$ref": "#/texts/54"
        },
        {
          "$ref": "#/texts/55"
        },
        {
          "$ref": "#/texts/56"
        },
        {
          "$ref": "#/texts/57"
        },
        {
          "$ref": "#/texts/58"
        },
        {
          "$ref": "#/texts/59"
        },
        {
          "$ref": "#/texts/60"
        },
        {
          "$ref": "#/texts/61"
        },
        {
          "$ref": "#/texts/62"
        },
        {
          "$ref": "#/texts/63"
        },
        {
          "$ref": "#/texts/64"
        },
        {
          "$ref": "#/texts/65"
        },
        {
          "$ref": "#/texts/66"
        },
        {
          "$ref": "#/texts/67"
        },
        {
          "$ref": "#/texts/68"
        },
        {
          "$ref": "#/texts/69"
        },
        {
          "$ref": "#/texts/70"
        },
        {
          "$ref": "#/texts/71"
        },
        {
          "$ref": "#/texts/72"
        },
        {
          "$ref": "#/texts/73"
        },
        {
          "$ref": "#/texts/74"
        },
        {
          "$ref": "#/texts/75"
        },
        {
          "$ref": "#/texts/76"
        },
        {
          "$ref": "#/texts/77"
        },
        {
          "$ref": "#/texts/78"
        },
        {
          "$ref": "#/texts/79"
        },
        {
          "$ref": "#/texts/80"
        },
        {
          "$ref": "#/texts/81"
        },
        {
          "$ref": "#/texts/82"
        },
        {
          "$ref": "#/texts/83"
        },
        {
          "$ref": "#/texts/84"
        },
        {
          "$ref": "#/texts/85"
        },
        {
          "$ref": "#/texts/86"
        },
        {
          "$ref": "#/texts/87"
        },
        {
          "$ref": "#/texts/88"
        }
      ],
      "label": "picture",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 81.55133056640625,
            "t": 720.5936889648438,
            "r": 529.0409545898438,
            "b": 598.0641479492188,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            535
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/51"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": []
    },
    {
      "self_ref": "#/pictures/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/107"
        },
        {
          "$ref": "#/texts/108"
        },
        {
          "$ref": "#/texts/109"
        },
        {
          "$ref": "#/texts/110"
        },
        {
          "$ref": "#/texts/111"
        },
        {
          "$ref": "#/texts/112"
        },
        {
          "$ref": "#/texts/113"
        },
        {
          "$ref": "#/texts/114"
        },
        {
          "$ref": "#/texts/115"
        },
        {
          "$ref": "#/texts/116"
        },
        {
          "$ref": "#/texts/117"
        },
        {
          "$ref": "#/texts/118"
        },
        {
          "$ref": "#/texts/119"
        },
        {
          "$ref": "#/texts/120"
        },
        {
          "$ref": "#/texts/121"
        },
        {
          "$ref": "#/texts/122"
        },
        {
          "$ref": "#/texts/123"
        },
        {
          "$ref": "#/texts/124"
        },
        {
          "$ref": "#/texts/125"
        },
        {
          "$ref": "#/texts/126"
        }
      ],
      "label": "picture",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 62.84389877319336,
            "t": 720.4003295898438,
            "r": 553.2965698242188,
            "b": 508.9026184082031,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            983
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/106"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": []
    },
    {
      "self_ref": "#/pictures/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/157"
        },
        {
          "$ref": "#/texts/158"
        },
        {
          "$ref": "#/texts/159"
        },
        {
          "$ref": "#/texts/160"
        },
        {
          "$ref": "#/texts/161"
        },
        {
          "$ref": "#/texts/162"
        },
        {
          "$ref": "#/texts/163"
        },
        {
          "$ref": "#/texts/164"
        },
        {
          "$ref": "#/texts/165"
        },
        {
          "$ref": "#/texts/166"
        }
      ],
      "label": "picture",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 58.03392028808594,
            "t": 721.36865234375,
            "r": 554.1041870117188,
            "b": 594.359375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            625
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/156"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": []
    },
    {
      "self_ref": "#/pictures/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/179"
        },
        {
          "$ref": "#/texts/180"
        },
        {
          "$ref": "#/texts/181"
        },
        {
          "$ref": "#/texts/182"
        },
        {
          "$ref": "#/texts/183"
        },
        {
          "$ref": "#/texts/184"
        },
        {
          "$ref": "#/texts/185"
        },
        {
          "$ref": "#/texts/186"
        },
        {
          "$ref": "#/texts/187"
        },
        {
          "$ref": "#/texts/188"
        },
        {
          "$ref": "#/texts/189"
        },
        {
          "$ref": "#/texts/190"
        },
        {
          "$ref": "#/texts/191"
        },
        {
          "$ref": "#/texts/192"
        },
        {
          "$ref": "#/texts/193"
        },
        {
          "$ref": "#/texts/194"
        },
        {
          "$ref": "#/texts/195"
        }
      ],
      "label": "picture",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 62.95893096923828,
            "t": 720.9797973632812,
            "r": 554.43408203125,
            "b": 467.3083190917969,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            264
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/178"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": []
    },
    {
      "self_ref": "#/pictures/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/197"
        },
        {
          "$ref": "#/texts/198"
        },
        {
          "$ref": "#/texts/199"
        },
        {
          "$ref": "#/texts/200"
        },
        {
          "$ref": "#/texts/201"
        },
        {
          "$ref": "#/texts/202"
        },
        {
          "$ref": "#/texts/203"
        },
        {
          "$ref": "#/texts/204"
        },
        {
          "$ref": "#/texts/205"
        },
        {
          "$ref": "#/texts/206"
        }
      ],
      "label": "picture",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 57.113040924072266,
            "t": 434.59649658203125,
            "r": 296.1763916015625,
            "b": 318.610595703125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            249
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/196"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": []
    }
  ],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 318.6004638671875,
            "t": 692.6519775390625,
            "r": 551.7731323242188,
            "b": 554.2814331054688,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/149"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 324.4521179199219,
              "t": 683.338134765625,
              "r": 351.9359436035156,
              "b": 677.5946044921875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Method",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 410.212646484375,
              "t": 688.4725952148438,
              "r": 546.2979736328125,
              "b": 672.5925903320312,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Model Setups Data Scale Frame Rate Resolution",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.4687805175781,
              "t": 664.247802734375,
              "r": 541.8206787109375,
              "b": 657.2205810546875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "DriveSim [30] 7h 5 Hz 80\u00d7160",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.4687805175781,
              "t": 653.02734375,
              "r": 436.2461242675781,
              "b": 646.033447265625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "DriveGAN [20]  160h",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 473.4413757324219,
              "t": 653.0606689453125,
              "r": 543.83740234375,
              "b": 647.2171020507812,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "8 Hz 256\u00d7256",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.4687805175781,
              "t": 643.0575561523438,
              "r": 543.895751953125,
              "b": 636.0303344726562,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "DriveDreamer [37] 5h 12 Hz 128\u00d7192",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.4687805175781,
              "t": 633.054443359375,
              "r": 543.8707885742188,
              "b": 626.0272216796875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Drive-WM [38] 5h 2 Hz 192\u00d7384",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.3771057128906,
              "t": 623.0513305664062,
              "r": 543.6456909179688,
              "b": 616.0241088867188,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "WoVoGen [27] 5h 2 Hz 256\u00d7448",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.46044921875,
              "t": 611.8226318359375,
              "r": 436.2461242675781,
              "b": 604.8370361328125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "ADriver-I [19]  300h",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 473.2246398925781,
              "t": 611.8643188476562,
              "r": 543.895751953125,
              "b": 606.020751953125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "2 Hz 256\u00d7512",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.6021728515625,
              "t": 601.8611450195312,
              "r": 543.6456909179688,
              "b": 594.8339233398438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 7,
            "end_row_offset_idx": 8,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "GenAD [42] 2000h 2 Hz 256\u00d7448",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 7,
            "end_row_offset_idx": 8,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.6021728515625,
              "t": 590.6324462890625,
              "r": 438.329833984375,
              "b": 583.6469116210938,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 8,
            "end_row_offset_idx": 9,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "GAIA-1 [17]  4700h",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 471.1403503417969,
              "t": 590.6741333007812,
              "r": 543.895751953125,
              "b": 584.83056640625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 8,
            "end_row_offset_idx": 9,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "25 Hz 288\u00d7512",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 9,
            "end_row_offset_idx": 10,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.4687805175781,
              "t": 580.6709594726562,
              "r": 545.955322265625,
              "b": 573.6437377929688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 9,
            "end_row_offset_idx": 10,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Vista [9] 1740h 10 Hz 576\u00d71024",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 324.2770690917969,
              "t": 565.2653198242188,
              "r": 449.01654052734375,
              "b": 557.8546142578125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 10,
            "end_row_offset_idx": 11,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "DrivingWorld (Ours) 120h+ 3336h",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 471.8155212402344,
              "t": 565.2986450195312,
              "r": 545.955322265625,
              "b": 559.455078125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 10,
            "end_row_offset_idx": 11,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "10 Hz 512\u00d71024",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 11,
        "num_cols": 2,
        "grid": [
          [
            {
              "bbox": {
                "l": 324.4521179199219,
                "t": 683.338134765625,
                "r": 351.9359436035156,
                "b": 677.5946044921875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Method",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 410.212646484375,
                "t": 688.4725952148438,
                "r": 546.2979736328125,
                "b": 672.5925903320312,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Model Setups Data Scale Frame Rate Resolution",
              "column_header": true,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.4687805175781,
                "t": 664.247802734375,
                "r": 541.8206787109375,
                "b": 657.2205810546875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "DriveSim [30] 7h 5 Hz 80\u00d7160",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.4687805175781,
                "t": 653.02734375,
                "r": 436.2461242675781,
                "b": 646.033447265625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "DriveGAN [20]  160h",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 473.4413757324219,
                "t": 653.0606689453125,
                "r": 543.83740234375,
                "b": 647.2171020507812,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "8 Hz 256\u00d7256",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.4687805175781,
                "t": 643.0575561523438,
                "r": 543.895751953125,
                "b": 636.0303344726562,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "DriveDreamer [37] 5h 12 Hz 128\u00d7192",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.4687805175781,
                "t": 633.054443359375,
                "r": 543.8707885742188,
                "b": 626.0272216796875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Drive-WM [38] 5h 2 Hz 192\u00d7384",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.3771057128906,
                "t": 623.0513305664062,
                "r": 543.6456909179688,
                "b": 616.0241088867188,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "WoVoGen [27] 5h 2 Hz 256\u00d7448",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.46044921875,
                "t": 611.8226318359375,
                "r": 436.2461242675781,
                "b": 604.8370361328125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "ADriver-I [19]  300h",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 473.2246398925781,
                "t": 611.8643188476562,
                "r": 543.895751953125,
                "b": 606.020751953125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "2 Hz 256\u00d7512",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.6021728515625,
                "t": 601.8611450195312,
                "r": 543.6456909179688,
                "b": 594.8339233398438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 7,
              "end_row_offset_idx": 8,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "GenAD [42] 2000h 2 Hz 256\u00d7448",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 7,
              "end_row_offset_idx": 8,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.6021728515625,
                "t": 590.6324462890625,
                "r": 438.329833984375,
                "b": 583.6469116210938,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 8,
              "end_row_offset_idx": 9,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "GAIA-1 [17]  4700h",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 471.1403503417969,
                "t": 590.6741333007812,
                "r": 543.895751953125,
                "b": 584.83056640625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 8,
              "end_row_offset_idx": 9,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "25 Hz 288\u00d7512",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 9,
              "end_row_offset_idx": 10,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 324.4687805175781,
                "t": 580.6709594726562,
                "r": 545.955322265625,
                "b": 573.6437377929688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 9,
              "end_row_offset_idx": 10,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Vista [9] 1740h 10 Hz 576\u00d71024",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 324.2770690917969,
                "t": 565.2653198242188,
                "r": 449.01654052734375,
                "b": 557.8546142578125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 10,
              "end_row_offset_idx": 11,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "DrivingWorld (Ours) 120h+ 3336h",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 471.8155212402344,
                "t": 565.2986450195312,
                "r": 545.955322265625,
                "b": 559.455078125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 10,
              "end_row_offset_idx": 11,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "10 Hz 512\u00d71024",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 68.9961166381836,
            "t": 529.1730346679688,
            "r": 540.0119018554688,
            "b": 490.73138427734375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/167"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 74.71680450439453,
              "t": 525.0764770507812,
              "r": 92.59790802001953,
              "b": 520.6734619140625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Metric",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 145.29052734375,
              "t": 525.0264892578125,
              "r": 192.49188232421875,
              "b": 519.7853393554688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "DriveDreamer [37]",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 200.5787353515625,
              "t": 524.982666015625,
              "r": 237.76693725585938,
              "b": 519.7853393554688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "WoVoGen [27]",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 245.9224090576172,
              "t": 525.0264892578125,
              "r": 284.7929992675781,
              "b": 519.7853393554688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "Drive-WM [38]",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 294.35565185546875,
              "t": 525.0264892578125,
              "r": 315.3889465332031,
              "b": 519.7853393554688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "Vista [9]",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 325.2022399902344,
              "t": 525.0264892578125,
              "r": 364.0728454589844,
              "b": 519.7853393554688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "DriveGAN [30]",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 372.32867431640625,
              "t": 524.982666015625,
              "r": 431.06927490234375,
              "b": 519.40380859375,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "GenAD (OpenDV) [42]",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 439.080810546875,
              "t": 525.0264892578125,
              "r": 493.31805419921875,
              "b": 519.4725952148438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 7,
            "end_col_offset_idx": 8,
            "text": "DrivingWorld (w/o P)",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 501.0797424316406,
              "t": 525.0264892578125,
              "r": 535.8099975585938,
              "b": 519.4725952148438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 8,
            "end_col_offset_idx": 9,
            "text": "DrivingWorld",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 74.72930908203125,
              "t": 514.449951171875,
              "r": 89.98344421386719,
              "b": 508.9085998535156,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "FID \u2193",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 163.8435516357422,
              "t": 514.4124145507812,
              "r": 174.38833618164062,
              "b": 510.02813720703125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "52.6",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 214.14700317382812,
              "t": 514.3873901367188,
              "r": 224.7042999267578,
              "b": 510.02813720703125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "27.6",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 260.8042297363281,
              "t": 514.4124145507812,
              "r": 270.7110595703125,
              "b": 510.02813720703125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "15.8",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 301.3638610839844,
              "t": 514.4124145507812,
              "r": 308.8377990722656,
              "b": 510.0343933105469,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "6.9",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 339.514892578125,
              "t": 514.3373413085938,
              "r": 350.15972900390625,
              "b": 510.02813720703125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "73.4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 397.09527587890625,
              "t": 514.4124145507812,
              "r": 407.17095947265625,
              "b": 510.02813720703125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "15.4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 461.5927734375,
              "t": 514.3873901367188,
              "r": 471.66845703125,
              "b": 510.02813720703125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 7,
            "end_col_offset_idx": 8,
            "text": "16.4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 514.6824340820312,
              "t": 514.3373413085938,
              "r": 522.2001342773438,
              "b": 510.0469055175781,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 8,
            "end_col_offset_idx": 9,
            "text": "7.4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 74.72930908203125,
              "t": 506.9442138671875,
              "r": 92.06642150878906,
              "b": 501.40289306640625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "FVD \u2193",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 162.1553192138672,
              "t": 506.9067077636719,
              "r": 176.00238037109375,
              "b": 502.5224304199219,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "452.0",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 212.47125244140625,
              "t": 506.8316650390625,
              "r": 226.14944458007812,
              "b": 502.54119873046875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "417.7",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 259.2410583496094,
              "t": 506.8316650390625,
              "r": 272.3000793457031,
              "b": 502.54119873046875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "122.7",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 299.8000793457031,
              "t": 506.9067077636719,
              "r": 310.4136657714844,
              "b": 502.5286865234375,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "89.4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 338.02679443359375,
              "t": 506.9067077636719,
              "r": 351.4736022949219,
              "b": 502.5224304199219,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "502.3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 395.5321044921875,
              "t": 506.8316650390625,
              "r": 408.7599792480469,
              "b": 502.5224304199219,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "184.0",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 460.02960205078125,
              "t": 506.8316650390625,
              "r": 473.2324523925781,
              "b": 502.54119873046875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 7,
            "end_col_offset_idx": 8,
            "text": "174.4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 513.1812133789062,
              "t": 506.8316650390625,
              "r": 523.6822509765625,
              "b": 502.4723815917969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 8,
            "end_col_offset_idx": 9,
            "text": "90.9",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 74.71680450439453,
              "t": 499.42034912109375,
              "r": 137.51019287109375,
              "b": 494.98602294921875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Max Duration / Frames",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 160.85140991210938,
              "t": 499.3265380859375,
              "r": 177.11264038085938,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "4s / 48",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 210.49769592285156,
              "t": 499.4015808105469,
              "r": 228.16616821289062,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "2.5s / 5",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 257.5931701660156,
              "t": 499.3765563964844,
              "r": 273.7230529785156,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "8s / 16",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 295.10614013671875,
              "t": 499.4015808105469,
              "r": 317.1963806152344,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "15s / 150",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 339.552734375,
              "t": 499.3265380859375,
              "r": 350.1475524902344,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "N/A",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 395.9543762207031,
              "t": 499.3265380859375,
              "r": 409.0884704589844,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "4s / 8",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 455.9549560546875,
              "t": 499.3265380859375,
              "r": 478.47052001953125,
              "b": 495.0173034667969,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 7,
            "end_col_offset_idx": 8,
            "text": "30s / 300",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 507.89996337890625,
              "t": 499.42034912109375,
              "r": 530.5656127929688,
              "b": 494.98602294921875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 8,
            "end_col_offset_idx": 9,
            "text": "40s / 400",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 4,
        "num_cols": 9,
        "grid": [
          [
            {
              "bbox": {
                "l": 74.71680450439453,
                "t": 525.0764770507812,
                "r": 92.59790802001953,
                "b": 520.6734619140625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Metric",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 145.29052734375,
                "t": 525.0264892578125,
                "r": 192.49188232421875,
                "b": 519.7853393554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "DriveDreamer [37]",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 200.5787353515625,
                "t": 524.982666015625,
                "r": 237.76693725585938,
                "b": 519.7853393554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "WoVoGen [27]",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 245.9224090576172,
                "t": 525.0264892578125,
                "r": 284.7929992675781,
                "b": 519.7853393554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "Drive-WM [38]",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 294.35565185546875,
                "t": 525.0264892578125,
                "r": 315.3889465332031,
                "b": 519.7853393554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "Vista [9]",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 325.2022399902344,
                "t": 525.0264892578125,
                "r": 364.0728454589844,
                "b": 519.7853393554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "DriveGAN [30]",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 372.32867431640625,
                "t": 524.982666015625,
                "r": 431.06927490234375,
                "b": 519.40380859375,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "GenAD (OpenDV) [42]",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 439.080810546875,
                "t": 525.0264892578125,
                "r": 493.31805419921875,
                "b": 519.4725952148438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 7,
              "end_col_offset_idx": 8,
              "text": "DrivingWorld (w/o P)",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 501.0797424316406,
                "t": 525.0264892578125,
                "r": 535.8099975585938,
                "b": 519.4725952148438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 8,
              "end_col_offset_idx": 9,
              "text": "DrivingWorld",
              "column_header": true,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 74.72930908203125,
                "t": 514.449951171875,
                "r": 89.98344421386719,
                "b": 508.9085998535156,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "FID \u2193",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 163.8435516357422,
                "t": 514.4124145507812,
                "r": 174.38833618164062,
                "b": 510.02813720703125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "52.6",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 214.14700317382812,
                "t": 514.3873901367188,
                "r": 224.7042999267578,
                "b": 510.02813720703125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "27.6",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 260.8042297363281,
                "t": 514.4124145507812,
                "r": 270.7110595703125,
                "b": 510.02813720703125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "15.8",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 301.3638610839844,
                "t": 514.4124145507812,
                "r": 308.8377990722656,
                "b": 510.0343933105469,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "6.9",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 339.514892578125,
                "t": 514.3373413085938,
                "r": 350.15972900390625,
                "b": 510.02813720703125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "73.4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 397.09527587890625,
                "t": 514.4124145507812,
                "r": 407.17095947265625,
                "b": 510.02813720703125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "15.4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 461.5927734375,
                "t": 514.3873901367188,
                "r": 471.66845703125,
                "b": 510.02813720703125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 7,
              "end_col_offset_idx": 8,
              "text": "16.4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 514.6824340820312,
                "t": 514.3373413085938,
                "r": 522.2001342773438,
                "b": 510.0469055175781,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 8,
              "end_col_offset_idx": 9,
              "text": "7.4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 74.72930908203125,
                "t": 506.9442138671875,
                "r": 92.06642150878906,
                "b": 501.40289306640625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "FVD \u2193",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 162.1553192138672,
                "t": 506.9067077636719,
                "r": 176.00238037109375,
                "b": 502.5224304199219,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "452.0",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 212.47125244140625,
                "t": 506.8316650390625,
                "r": 226.14944458007812,
                "b": 502.54119873046875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "417.7",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 259.2410583496094,
                "t": 506.8316650390625,
                "r": 272.3000793457031,
                "b": 502.54119873046875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "122.7",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 299.8000793457031,
                "t": 506.9067077636719,
                "r": 310.4136657714844,
                "b": 502.5286865234375,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "89.4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 338.02679443359375,
                "t": 506.9067077636719,
                "r": 351.4736022949219,
                "b": 502.5224304199219,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "502.3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 395.5321044921875,
                "t": 506.8316650390625,
                "r": 408.7599792480469,
                "b": 502.5224304199219,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "184.0",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 460.02960205078125,
                "t": 506.8316650390625,
                "r": 473.2324523925781,
                "b": 502.54119873046875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 7,
              "end_col_offset_idx": 8,
              "text": "174.4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 513.1812133789062,
                "t": 506.8316650390625,
                "r": 523.6822509765625,
                "b": 502.4723815917969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 8,
              "end_col_offset_idx": 9,
              "text": "90.9",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 74.71680450439453,
                "t": 499.42034912109375,
                "r": 137.51019287109375,
                "b": 494.98602294921875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Max Duration / Frames",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 160.85140991210938,
                "t": 499.3265380859375,
                "r": 177.11264038085938,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "4s / 48",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 210.49769592285156,
                "t": 499.4015808105469,
                "r": 228.16616821289062,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "2.5s / 5",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 257.5931701660156,
                "t": 499.3765563964844,
                "r": 273.7230529785156,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "8s / 16",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 295.10614013671875,
                "t": 499.4015808105469,
                "r": 317.1963806152344,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "15s / 150",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 339.552734375,
                "t": 499.3265380859375,
                "r": 350.1475524902344,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "N/A",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 395.9543762207031,
                "t": 499.3265380859375,
                "r": 409.0884704589844,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "4s / 8",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 455.9549560546875,
                "t": 499.3265380859375,
                "r": 478.47052001953125,
                "b": 495.0173034667969,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 7,
              "end_col_offset_idx": 8,
              "text": "30s / 300",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 507.89996337890625,
                "t": 499.42034912109375,
                "r": 530.5656127929688,
                "b": 494.98602294921875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 8,
              "end_col_offset_idx": 9,
              "text": "40s / 400",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 59.20074462890625,
            "t": 428.855712890625,
            "r": 292.7929382324219,
            "b": 366.83294677734375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 65.35836029052734,
              "t": 424.4112854003906,
              "r": 121.2481689453125,
              "b": 417.60028076171875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "VQVAE Methods",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 160.04653930664062,
              "t": 424.5063171386719,
              "r": 188.2194061279297,
              "b": 417.5052490234375,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "FVD12 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 198.03878784179688,
              "t": 424.5063171386719,
              "r": 287.79962158203125,
              "b": 417.4814758300781,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "FID \u2193  PSNR \u2191  LPIPS \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 65.35836029052734,
              "t": 414.5350036621094,
              "r": 95.35843658447266,
              "b": 407.95367431640625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "VAR [34]",
            "column_header": false,
            "row_header": true,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 164.18528747558594,
              "t": 414.6300354003906,
              "r": 285.15972900390625,
              "b": 409.0782775878906,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "164.66 11.75 22.35 0.2018",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 65.35834503173828,
              "t": 405.0314025878906,
              "r": 104.3394546508789,
              "b": 398.2758483886719,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "VQGAN [8]",
            "column_header": false,
            "row_header": true,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 164.18528747558594,
              "t": 405.1264343261719,
              "r": 285.3973388671875,
              "b": 399.5746765136719,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "156.58 8.46 21.52 0.2602",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 65.32666778564453,
              "t": 395.5832214355469,
              "r": 115.7756118774414,
              "b": 388.9464416503906,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Llama-Gen [33]",
            "column_header": false,
            "row_header": true,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 165.53985595703125,
              "t": 395.622802734375,
              "r": 285.37384033203125,
              "b": 390.0076904296875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "57.78 5.99 22.31 0.2054",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 65.32666778564453,
              "t": 386.07958984375,
              "r": 150.06028747558594,
              "b": 379.44281005859375,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Llama-Gen [33] Finetuned",
            "column_header": false,
            "row_header": true,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 165.52401733398438,
              "t": 386.1191711425781,
              "r": 285.2708740234375,
              "b": 380.5040588378906,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "20.33 5.19 22.71 0.1909",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 65.36626434326172,
              "t": 376.57598876953125,
              "r": 139.08346557617188,
              "b": 369.45611572265625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Temporal-aware (Ours)",
            "column_header": false,
            "row_header": true,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 165.80120849609375,
              "t": 376.61553955078125,
              "r": 285.37384033203125,
              "b": 371.06378173828125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "14.66 4.29 23.82 0.1828",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 6,
        "num_cols": 3,
        "grid": [
          [
            {
              "bbox": {
                "l": 65.35836029052734,
                "t": 424.4112854003906,
                "r": 121.2481689453125,
                "b": 417.60028076171875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "VQVAE Methods",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 160.04653930664062,
                "t": 424.5063171386719,
                "r": 188.2194061279297,
                "b": 417.5052490234375,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "FVD12 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 198.03878784179688,
                "t": 424.5063171386719,
                "r": 287.79962158203125,
                "b": 417.4814758300781,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "FID \u2193  PSNR \u2191  LPIPS \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 65.35836029052734,
                "t": 414.5350036621094,
                "r": 95.35843658447266,
                "b": 407.95367431640625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "VAR [34]",
              "column_header": false,
              "row_header": true,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 164.18528747558594,
                "t": 414.6300354003906,
                "r": 285.15972900390625,
                "b": 409.0782775878906,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "164.66 11.75 22.35 0.2018",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 65.35834503173828,
                "t": 405.0314025878906,
                "r": 104.3394546508789,
                "b": 398.2758483886719,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "VQGAN [8]",
              "column_header": false,
              "row_header": true,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 164.18528747558594,
                "t": 405.1264343261719,
                "r": 285.3973388671875,
                "b": 399.5746765136719,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "156.58 8.46 21.52 0.2602",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 65.32666778564453,
                "t": 395.5832214355469,
                "r": 115.7756118774414,
                "b": 388.9464416503906,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Llama-Gen [33]",
              "column_header": false,
              "row_header": true,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 165.53985595703125,
                "t": 395.622802734375,
                "r": 285.37384033203125,
                "b": 390.0076904296875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "57.78 5.99 22.31 0.2054",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 65.32666778564453,
                "t": 386.07958984375,
                "r": 150.06028747558594,
                "b": 379.44281005859375,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Llama-Gen [33] Finetuned",
              "column_header": false,
              "row_header": true,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 165.52401733398438,
                "t": 386.1191711425781,
                "r": 285.2708740234375,
                "b": 380.5040588378906,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "20.33 5.19 22.71 0.1909",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 65.36626434326172,
                "t": 376.57598876953125,
                "r": 139.08346557617188,
                "b": 369.45611572265625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Temporal-aware (Ours)",
              "column_header": false,
              "row_header": true,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 165.80120849609375,
                "t": 376.61553955078125,
                "r": 285.37384033203125,
                "b": 371.06378173828125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "14.66 4.29 23.82 0.1828",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 89.33489227294922,
            "t": 239.81439208984375,
            "r": 263.4061279296875,
            "b": 200.38861083984375,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/169"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 95.20700073242188,
              "t": 235.42550659179688,
              "r": 178.04965209960938,
              "b": 228.06704711914062,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Methods FVD 10  \u2193",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 188.37026977539062,
              "t": 235.42550659179688,
              "r": 257.912109375,
              "b": 228.06704711914062,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "FVD25 \u2193  FVD40 \u2193",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 95.24029541015625,
              "t": 221.19915771484375,
              "r": 254.42059326171875,
              "b": 213.6658935546875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "w/o Masking 449.40 595.49 662.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 95.39012908935547,
              "t": 211.11050415039062,
              "r": 110.9477767944336,
              "b": 205.37522888183594,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Ours",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 152.02281188964844,
              "t": 211.21038818359375,
              "r": 254.41696166992188,
              "b": 205.37522888183594,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "445.22 574.57 637.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 3,
        "num_cols": 2,
        "grid": [
          [
            {
              "bbox": {
                "l": 95.20700073242188,
                "t": 235.42550659179688,
                "r": 178.04965209960938,
                "b": 228.06704711914062,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Methods FVD 10  \u2193",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 188.37026977539062,
                "t": 235.42550659179688,
                "r": 257.912109375,
                "b": 228.06704711914062,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "FVD25 \u2193  FVD40 \u2193",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 95.24029541015625,
                "t": 221.19915771484375,
                "r": 254.42059326171875,
                "b": 213.6658935546875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "w/o Masking 449.40 595.49 662.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 95.39012908935547,
                "t": 211.11050415039062,
                "r": 110.9477767944336,
                "b": 205.37522888183594,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Ours",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 152.02281188964844,
                "t": 211.21038818359375,
                "r": 254.41696166992188,
                "b": 205.37522888183594,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "445.22 574.57 637.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 89.19808959960938,
            "t": 217.46337890625,
            "r": 263.6359558105469,
            "b": 175.6842041015625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/207"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 95.42804718017578,
              "t": 211.93658447265625,
              "r": 175.3075714111328,
              "b": 204.34632873535156,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Methods FVD 10  \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 185.95420837402344,
              "t": 211.93658447265625,
              "r": 257.6866455078125,
              "b": 204.34632873535156,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "FVD25 \u2193  FVD40 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 95.59976959228516,
              "t": 197.2620849609375,
              "r": 255.90536499023438,
              "b": 190.02386474609375,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "GPT-2 [29] 2976.97 3505.22 4017.15",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 95.616943359375,
              "t": 186.85479736328125,
              "r": 111.66468048095703,
              "b": 180.93885803222656,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Ours",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 148.4617462158203,
              "t": 186.95782470703125,
              "r": 254.0814208984375,
              "b": 180.93885803222656,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "445.22 574.57 637.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 3,
        "num_cols": 2,
        "grid": [
          [
            {
              "bbox": {
                "l": 95.42804718017578,
                "t": 211.93658447265625,
                "r": 175.3075714111328,
                "b": 204.34632873535156,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Methods FVD 10  \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 185.95420837402344,
                "t": 211.93658447265625,
                "r": 257.6866455078125,
                "b": 204.34632873535156,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "FVD25 \u2193  FVD40 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 95.59976959228516,
                "t": 197.2620849609375,
                "r": 255.90536499023438,
                "b": 190.02386474609375,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "GPT-2 [29] 2976.97 3505.22 4017.15",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 95.616943359375,
                "t": 186.85479736328125,
                "r": 111.66468048095703,
                "b": 180.93885803222656,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Ours",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 148.4617462158203,
                "t": 186.95782470703125,
                "r": 254.0814208984375,
                "b": 180.93885803222656,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "445.22 574.57 637.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 316.05462646484375,
            "t": 391.179931640625,
            "r": 554.0260009765625,
            "b": 359.40753173828125,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/209"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 321.35626220703125,
              "t": 386.9863586425781,
              "r": 543.193115234375,
              "b": 382.2295837402344,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Num. of frames 5 6 7 8 9 10 15",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 321.4904479980469,
              "t": 375.5670166015625,
              "r": 547.9095458984375,
              "b": 369.9112243652344,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "GPT-2 [29] 31.555 39.305 47.237 55.604 66.169 77.559 OOM",
            "column_header": false,
            "row_header": true,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 321.5038757324219,
              "t": 367.51617431640625,
              "r": 549.023193359375,
              "b": 362.7593994140625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Ours 21.927 24.815 26.987 29.877 31.219 34.325 45.873",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 3,
        "num_cols": 2,
        "grid": [
          [
            {
              "bbox": {
                "l": 321.35626220703125,
                "t": 386.9863586425781,
                "r": 543.193115234375,
                "b": 382.2295837402344,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Num. of frames 5 6 7 8 9 10 15",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 321.4904479980469,
                "t": 375.5670166015625,
                "r": 547.9095458984375,
                "b": 369.9112243652344,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "GPT-2 [29] 31.555 39.305 47.237 55.604 66.169 77.559 OOM",
              "column_header": false,
              "row_header": true,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 321.5038757324219,
                "t": 367.51617431640625,
                "r": 549.023193359375,
                "b": 362.7593994140625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Ours 21.927 24.815 26.987 29.877 31.219 34.325 45.873",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 58.170997619628906,
            "t": 689.1072387695312,
            "r": 294.56097412109375,
            "b": 635.556640625,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/293"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 62.49105453491211,
              "t": 684.8347778320312,
              "r": 107.77496337890625,
              "b": 672.7862548828125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Number of Condition Frames",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 115.73680877685547,
              "t": 677.2854614257812,
              "r": 290.76416015625,
              "b": 671.6678466796875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "FVD 10  \u2193  FVD40 \u2193  FVD10 \u2193  FVD40 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 145.47512817382812,
              "t": 684.8347778320312,
              "r": 258.2845458984375,
              "b": 671.6678466796875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Nuscenes Nuplan FVD25 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 237.67459106445312,
              "t": 677.2854614257812,
              "r": 260.2795715332031,
              "b": 671.6678466796875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "FVD25 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 83.7884750366211,
              "t": 666.4241943359375,
              "r": 287.8582763671875,
              "b": 661.918701171875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "5 475.14 802.35 1113.81 494.86 597.95 679.05",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 82.70159149169922,
              "t": 658.798583984375,
              "r": 288.09954833984375,
              "b": 654.2930908203125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "10 448.93 719.57 965.62 449.29 577.54 646.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 82.70159149169922,
              "t": 651.1729125976562,
              "r": 288.09954833984375,
              "b": 646.6674194335938,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "15 440.27 695.26 933.13 445.22 574.57 637.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 82.18685913085938,
              "t": 643.5472412109375,
              "r": 87.95696258544922,
              "b": 639.092529296875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "25",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 118.45399475097656,
              "t": 643.5472412109375,
              "r": 288.1001892089844,
              "b": 639.092529296875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "360.55 546.11 721.56 400.94 512.73 580.10",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 4,
        "grid": [
          [
            {
              "bbox": {
                "l": 62.49105453491211,
                "t": 684.8347778320312,
                "r": 107.77496337890625,
                "b": 672.7862548828125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Number of Condition Frames",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 115.73680877685547,
                "t": 677.2854614257812,
                "r": 290.76416015625,
                "b": 671.6678466796875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "FVD 10  \u2193  FVD40 \u2193  FVD10 \u2193  FVD40 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 145.47512817382812,
                "t": 684.8347778320312,
                "r": 258.2845458984375,
                "b": 671.6678466796875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Nuscenes Nuplan FVD25 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 237.67459106445312,
                "t": 677.2854614257812,
                "r": 260.2795715332031,
                "b": 671.6678466796875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "FVD25 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 83.7884750366211,
                "t": 666.4241943359375,
                "r": 287.8582763671875,
                "b": 661.918701171875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "5 475.14 802.35 1113.81 494.86 597.95 679.05",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 82.70159149169922,
                "t": 658.798583984375,
                "r": 288.09954833984375,
                "b": 654.2930908203125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "10 448.93 719.57 965.62 449.29 577.54 646.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 82.70159149169922,
                "t": 651.1729125976562,
                "r": 288.09954833984375,
                "b": 646.6674194335938,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "15 440.27 695.26 933.13 445.22 574.57 637.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 82.18685913085938,
                "t": 643.5472412109375,
                "r": 87.95696258544922,
                "b": 639.092529296875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "25",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 118.45399475097656,
                "t": 643.5472412109375,
                "r": 288.1001892089844,
                "b": 639.092529296875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "360.55 546.11 721.56 400.94 512.73 580.10",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/7",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 316.57366943359375,
            "t": 655.5452880859375,
            "r": 553.49169921875,
            "b": 614.1683959960938,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/303"
        },
        {
          "$ref": "#/texts/304"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 321.568603515625,
              "t": 647.3622436523438,
              "r": 345.8906555175781,
              "b": 642.4794921875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Methods",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 376.6983642578125,
              "t": 643.2136840820312,
              "r": 413.67987060546875,
              "b": 636.9761962890625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "\u2193  FVD25 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 354.7317199707031,
              "t": 643.2136840820312,
              "r": 447.52783203125,
              "b": 636.9761962890625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "FVD 10  FVD40 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 387.7510070800781,
              "t": 651.59619140625,
              "r": 515.2252197265625,
              "b": 636.9761962890625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "Nuscenes Nuplan FVD10 \u2193  FVD25 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 523.9736328125,
              "t": 643.2136840820312,
              "r": 549.0731811523438,
              "b": 636.9761962890625,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "FVD40 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 321.5968017578125,
              "t": 631.15380859375,
              "r": 547.296875,
              "b": 626.1510620117188,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "w/o AR 523.53 1052.30 1601.36 525.04 729.75 1007.91",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 321.72381591796875,
              "t": 622.6019897460938,
              "r": 334.9114990234375,
              "b": 617.7404174804688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Ours",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 357.7691955566406,
              "t": 622.6867065429688,
              "r": 546.115234375,
              "b": 617.7404174804688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "440.27 695.26 933.13 445.22 574.57 637.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 3,
        "num_cols": 5,
        "grid": [
          [
            {
              "bbox": {
                "l": 321.568603515625,
                "t": 647.3622436523438,
                "r": 345.8906555175781,
                "b": 642.4794921875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Methods",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 376.6983642578125,
                "t": 643.2136840820312,
                "r": 413.67987060546875,
                "b": 636.9761962890625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "\u2193  FVD25 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 354.7317199707031,
                "t": 643.2136840820312,
                "r": 447.52783203125,
                "b": 636.9761962890625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "FVD 10  FVD40 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 387.7510070800781,
                "t": 651.59619140625,
                "r": 515.2252197265625,
                "b": 636.9761962890625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "Nuscenes Nuplan FVD10 \u2193  FVD25 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 523.9736328125,
                "t": 643.2136840820312,
                "r": 549.0731811523438,
                "b": 636.9761962890625,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "FVD40 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 321.5968017578125,
                "t": 631.15380859375,
                "r": 547.296875,
                "b": 626.1510620117188,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "w/o AR 523.53 1052.30 1601.36 525.04 729.75 1007.91",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 321.72381591796875,
                "t": 622.6019897460938,
                "r": 334.9114990234375,
                "b": 617.7404174804688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Ours",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 357.7691955566406,
                "t": 622.6867065429688,
                "r": 546.115234375,
                "b": 617.7404174804688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "440.27 695.26 933.13 445.22 574.57 637.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    },
    {
      "self_ref": "#/tables/8",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "label": "table",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 316.87103271484375,
            "t": 558.23486328125,
            "r": 553.7577514648438,
            "b": 507.2872009277344,
            "coord_origin": "BOTTOMLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "bbox": {
              "l": 321.568603515625,
              "t": 549.8408813476562,
              "r": 345.8906555175781,
              "b": 544.9581298828125,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Methods",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 387.7510070800781,
              "t": 554.0740356445312,
              "r": 513.00927734375,
              "b": 547.7306518554688,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 3,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 4,
            "text": "Nuscenes Nuplan",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 354.7317199707031,
              "t": 545.6915283203125,
              "r": 379.83123779296875,
              "b": 539.4540405273438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "FVD 10  \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 388.5796813964844,
              "t": 545.6915283203125,
              "r": 515.2252197265625,
              "b": 539.4540405273438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "FVD25 \u2193  FVD40 \u2193  FVD25 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 456.2770080566406,
              "t": 545.6915283203125,
              "r": 481.37652587890625,
              "b": 539.4540405273438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "FVD10 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 523.9736328125,
              "t": 545.6915283203125,
              "r": 549.0731811523438,
              "b": 539.4540405273438,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "FVD40 \u2193",
            "column_header": true,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 322.2671203613281,
              "t": 533.6323852539062,
              "r": 547.29736328125,
              "b": 528.629638671875,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "10M 654.95 1248.53 1817.82 816.39 1003.03 1262.31",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 322.2671203613281,
              "t": 525.165283203125,
              "r": 545.5333251953125,
              "b": 520.1625366210938,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "100M 463.72 809.02 1120.30 481.25 609.20 915.01",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 322.2671203613281,
              "t": 516.6134643554688,
              "r": 329.1961364746094,
              "b": 511.8435974121094,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "1B",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "bbox": {
              "l": 357.7691955566406,
              "t": 516.6981201171875,
              "r": 546.115234375,
              "b": 511.7518615722656,
              "coord_origin": "BOTTOMLEFT"
            },
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "440.27 695.26 933.13 445.22 574.57 637.60",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 6,
        "grid": [
          [
            {
              "bbox": {
                "l": 321.568603515625,
                "t": 549.8408813476562,
                "r": 345.8906555175781,
                "b": 544.9581298828125,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Methods",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 387.7510070800781,
                "t": 554.0740356445312,
                "r": 513.00927734375,
                "b": 547.7306518554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 3,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 4,
              "text": "Nuscenes Nuplan",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 387.7510070800781,
                "t": 554.0740356445312,
                "r": 513.00927734375,
                "b": 547.7306518554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 3,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 4,
              "text": "Nuscenes Nuplan",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 387.7510070800781,
                "t": 554.0740356445312,
                "r": 513.00927734375,
                "b": 547.7306518554688,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 3,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 4,
              "text": "Nuscenes Nuplan",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 354.7317199707031,
                "t": 545.6915283203125,
                "r": 379.83123779296875,
                "b": 539.4540405273438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "FVD 10  \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 388.5796813964844,
                "t": 545.6915283203125,
                "r": 515.2252197265625,
                "b": 539.4540405273438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "FVD25 \u2193  FVD40 \u2193  FVD25 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 456.2770080566406,
                "t": 545.6915283203125,
                "r": 481.37652587890625,
                "b": 539.4540405273438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "FVD10 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 523.9736328125,
                "t": 545.6915283203125,
                "r": 549.0731811523438,
                "b": 539.4540405273438,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "FVD40 \u2193",
              "column_header": true,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 322.2671203613281,
                "t": 533.6323852539062,
                "r": 547.29736328125,
                "b": 528.629638671875,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "10M 654.95 1248.53 1817.82 816.39 1003.03 1262.31",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 322.2671203613281,
                "t": 525.165283203125,
                "r": 545.5333251953125,
                "b": 520.1625366210938,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "100M 463.72 809.02 1120.30 481.25 609.20 915.01",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "bbox": {
                "l": 322.2671203613281,
                "t": 516.6134643554688,
                "r": 329.1961364746094,
                "b": 511.8435974121094,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "1B",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "bbox": {
                "l": 357.7691955566406,
                "t": 516.6981201171875,
                "r": 546.115234375,
                "b": 511.7518615722656,
                "coord_origin": "BOTTOMLEFT"
              },
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "440.27 695.26 933.13 445.22 574.57 637.60",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      }
    }
  ],
  "key_value_items": [],
  "pages": {
    "1": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 1
    },
    "2": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 2
    },
    "3": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 3
    },
    "4": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 4
    },
    "5": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 5
    },
    "6": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 6
    },
    "7": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 7
    },
    "8": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 8
    },
    "9": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 9
    },
    "10": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 10
    },
    "11": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 11
    },
    "12": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 12
    },
    "13": {
      "size": {
        "width": 612.0,
        "height": 792.0
      },
      "page_no": 13
    }
  }
}