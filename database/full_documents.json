{
  "2412.19505v2": {
    "title": "DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT",
    "authors": "Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan",
    "pdf_url": "http://arxiv.org/pdf/2412.19505v2",
    "abstract": "Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token * Contributed equally. † Project leader: yvanwy@outlook.com ‡ Corresponding author: xxlong@connect.hku.hk.",
    "conclusion": "and Future Work In conclusion, DrivingWorld addressed the limitations of previous video generation models in autonomous driving by leveraging a GPT-style framework to produce longer, high-fidelity video predictions with improved generalization. Unlike traditional methods that struggled with coherence in long sequences or relied heavily on labeled data, DrivingWorld generated realistic, structured video sequences while enabling precise action control. Compared to the classic GPT structure, our proposed spatial-temporal GPT structure adopted next-state prediction strategy to model temporal coherence between consecutive frames and then applied next-token prediction strategy to capture spatial information within each frame. Looking ahead, we plan to incorporate"
  },
  "2412.18607v1": {
    "title": "DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers",
    "authors": "Yuntao Chen, Yuqi Wang, Zhaoxiang Zhang",
    "pdf_url": "http://arxiv.org/pdf/2412.18607v1",
    "abstract": "World model-based searching and planning are widely recognized as a promising path toward human-level physical intelligence. However, current driving world models primarily rely on video diffusion models, which specialize in visual generation but lack the flexibility to incorporate other modalities like action. In contrast, autoregressive transformers have demonstrated exceptional capability in modeling multimodal data. Our work aims to unify both driving model simulation and trajectory planning into a single sequence modeling problem. We introduce a multimodal driving language based on interleaved image and action tokens, and develop DrivingGPT to learn joint world modeling and planning through standard next-token prediction. Our DrivingGPT demonstrates strong performance in both action-conditioned video generation and end-to-end planning, outperforming strong baselines on large-scale nuPlan",
    "conclusion": "In this work, we propose a novel multi-modal driving language that effectively unifies the visual world modeling and trajectory planning into a sequence modeling task. We design a DrivingGPT that could jointly learn to generate image and action tokens for both tasks. Experiments and ablation studies on large-scale nuPlan and NAVSIM bench-"
  }
}